{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embeddings (Part 2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vacV4BIFI8l",
        "colab_type": "code",
        "outputId": "d8700be7-1b2e-4540-ea83-c14838127064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip install -q --upgrade nltk gensim bokeh pandas\n",
        "\n",
        "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
        "!unzip quora.zip\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 519.5MB 32kB/s \n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.60 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 24.2MB 44.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1MB 45.7MB/s \n",
            "\u001b[?25h  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.25.0; python_version >= \"3.0\", but you'll have pandas 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.60 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "Archive:  quora.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIFSTdJG95SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbpWIAreB6ky",
        "colab_type": "text"
      },
      "source": [
        "# Введение в PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M0mMOadG8aZ",
        "colab_type": "text"
      },
      "source": [
        "PyTorch - это один из самых известных фреймворков для работы с нейронными сетями.\n",
        "\n",
        "Почему именно он? Ну, он няшен, питоняч и проще в отладке - по сравнению с монстрами типа tensoflow (хотя tf 2.0 с eager execution будет примерно таким же).\n",
        "\n",
        "И вообще, мы тут не фреймворки, а сеточки учить собирались :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsScdJ7DLZCm",
        "colab_type": "text"
      },
      "source": [
        "## Автоматическое дифференцирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY9FHLM-M4aW",
        "colab_type": "text"
      },
      "source": [
        "### Графы вычислений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkvCloDpNXdH",
        "colab_type": "text"
      },
      "source": [
        "Графы вычислений - это такой удобный способ быстро считать градиенты сложных-пресложных функций.\n",
        "\n",
        "Например, функция\n",
        "\n",
        "$$f = (x + y) \\cdot z$$\n",
        "\n",
        "представится графом\n",
        "\n",
        "![graph](https://image.ibb.co/mWM0Lx/1_6o_Utr7_ENFHOK7_J4l_XJtw1g.png)  \n",
        "*From [Backpropagation, Intuitions - CS231n](http://cs231n.github.io/optimization-2/)*\n",
        "\n",
        "**Задание** Зададим значения $x, y, z$ (зеленым на картинке). Как посчитать $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$? (*Вспоминаем, что такое backpropagation*)\n",
        "\n",
        "В PyTorch такие вычисления делаются очень просто.\n",
        "\n",
        "Сначала определяется функция - просто последовательность операций:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw4ASRktLdO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor(-2., requires_grad=True)\n",
        "y = torch.tensor(5., requires_grad=True)\n",
        "z = torch.tensor(-4., requires_grad=True)\n",
        "\n",
        "q = x + y\n",
        "f = q * z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-78COM99N8YL",
        "colab_type": "text"
      },
      "source": [
        "А затем говорим ей: \"Посчитай градиенты, пожалуйста\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FOlPMIQMfbq",
        "colab_type": "code",
        "outputId": "0797f81d-0445-4697-9324-7e40fb71b8d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "f.backward()\n",
        "\n",
        "print('df/dz =', z.grad)\n",
        "print('df/dx =', x.grad)\n",
        "print('df/dy =', y.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df/dz = tensor(3.)\n",
            "df/dx = tensor(-4.)\n",
            "df/dy = tensor(-4.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JotDf1naGU-R",
        "colab_type": "text"
      },
      "source": [
        "Вызов метода `backward()` вычисляет градиенты для всех тензоров, у которых `requires_grad == True`.\n",
        "\n",
        "Есть еще альтернативный способ не вычислять градиенты - пользоваться менеджерами контекста ([Locally disabling gradient computation](https://pytorch.org/docs/stable/autograd.html#locally-disabling-gradient-computation)):\n",
        "```python\n",
        "torch.autograd.no_grad()\n",
        "torch.autograd.enable_grad()\n",
        "torch.autograd.set_grad_enabled(mode)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQEJeqfnJPpA",
        "colab_type": "code",
        "outputId": "aa407edd-2a2e-421e-93e9-376d25932d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "with torch.autograd.no_grad():\n",
        "    x = torch.tensor(-2., requires_grad=True)\n",
        "    y = torch.tensor(5., requires_grad=True)\n",
        "    q = x + y\n",
        "\n",
        "z = torch.tensor(-4., requires_grad=True)\n",
        "f = q * z\n",
        "\n",
        "f.backward()\n",
        "\n",
        "print('df/dz =', z.grad)\n",
        "print('df/dx =', x.grad)\n",
        "print('df/dy =', y.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df/dz = tensor(3.)\n",
            "df/dx = None\n",
            "df/dy = None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSiB1CGyJMzt",
        "colab_type": "text"
      },
      "source": [
        "Подробнее о том, как работает autograd, можно почитать здесь: [Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html).\n",
        "\n",
        "В целом, любой тензор в pytorch - аналог многомерных матриц в numpy.\n",
        "\n",
        "Он содержит данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY2CcCw2Gmgq",
        "colab_type": "code",
        "outputId": "673737ab-a091-4206-f2c2-dce4b3dd8f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x.data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-2.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYxD8N_9GpJl",
        "colab_type": "text"
      },
      "source": [
        "Накопленный градиент:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYCD5P24GufX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwLx4szvGwMb",
        "colab_type": "text"
      },
      "source": [
        "Функцию, как градиент считать:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTfGdUF_GzV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q.grad_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgK1Esa6HHAB",
        "colab_type": "text"
      },
      "source": [
        "И всякую дополнительную метаинформацию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nazaer0AG4pL",
        "colab_type": "code",
        "outputId": "030c7207-0c44-4969-ab8a-787e039526d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x.type(), x.shape, x.device, x.layout"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('torch.FloatTensor', torch.Size([]), device(type='cpu'), torch.strided)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvLFlc4iQOQv",
        "colab_type": "text"
      },
      "source": [
        "Зачем... У меня один вопрос - зачем вот это вот нам нужно?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlhLBWwHG3Xe",
        "colab_type": "text"
      },
      "source": [
        "### Задача для разминки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaqtIIvJOEut",
        "colab_type": "text"
      },
      "source": [
        "Чтобы разобраться - решим простенькую задачу на линейную регрессию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDZpEHF8AKH2",
        "colab_type": "code",
        "outputId": "00fb80f5-9815-47f4-c074-7c294782aa20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "w_orig, b_orig = 2.6, -0.4\n",
        "\n",
        "X = np.random.rand(100) * 10. - 5.\n",
        "y_orig = w_orig * X + b_orig\n",
        "\n",
        "y = y_orig + np.random.randn(100)\n",
        "\n",
        "plt.plot(X, y, '.')\n",
        "plt.plot(X, y_orig)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZgp9/OICAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2K5MVtiSGuC",
        "colab_type": "text"
      },
      "source": [
        "Хочется прикрутить сюда backpropagation, да.\n",
        "\n",
        "Есть два параметра $w$ и $b$ - их нужно подобрать такими, чтобы они были как можно ближе к исходным $w_{orig}, b_{orig}$.\n",
        "\n",
        "Что будем оптимизировать? Оптимизировать будем MSE:\n",
        "$$J(w, b) = \\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - y_i(w, b)||^2 =\\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - (w \\cdot x_i + b)||^2. $$\n",
        "\n",
        "С такой функций потерь можем запустить простой градиентный спуск (даже не стохастический пока):\n",
        "$$w_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial w}(w_t, b_t)$$\n",
        "$$b_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial b}(w_t, b_t)$$\n",
        "\n",
        "**Задание** Реализовать оптимизацию на чистом numpy.\n",
        "\n",
        "Для этого нужно:\n",
        "1. Посчитать значение функции на прямом проходе: $y(w, b) = w \\cdot x + b$;\n",
        "2. Подумать и посчитать градиенты $\\frac{\\partial J}{\\partial w}, \\frac{\\partial J}{\\partial b}$ на обратном проходе;\n",
        "3. Сдвинуть $w, b$ по антиградиентам."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKbqTNVXFB3A",
        "colab_type": "code",
        "outputId": "b477dbb9-912f-4d9b-914f-6e6f83b0f637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "def display_progress(epoch, loss, w, b, X, y, y_pred):\n",
        "    clear_output(True)\n",
        "    print('Epoch = {}, Loss = {}, w = {}, b = {}'.format(epoch, loss, w, b))\n",
        "    plt.plot(X, y, '.')\n",
        "    plt.plot(X, y_pred)\n",
        "    plt.show()\n",
        "    time.sleep(1)\n",
        "\n",
        "\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "\n",
        "\n",
        "\n",
        "alpha = 0.01\n",
        "\n",
        "for i in range(100):\n",
        "    y_pred = w * X + b\n",
        "\n",
        "    loss = ((y-y_pred)**2).sum()/len(y_pred)\n",
        "\n",
        "    w_grad = -2 / (y.shape[0]) * np.sum(X * (y - y_pred))\n",
        "    b_grad = -2 / (y.shape[0]) * np.sum(y - y_pred)\n",
        "\n",
        "    w -= alpha * w_grad\n",
        "    b -= alpha * b_grad\n",
        "    \n",
        "    if (i + 1) % 5 == 0:\n",
        "        display_progress(i + 1, loss, w, b, X, y, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch = 100, Loss = 0.7178397998887753, w = 2.5834364451199177, b = -0.031443890427106276\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8WgWrF4C2WK",
        "colab_type": "text"
      },
      "source": [
        "На PyTorch то же самое сделать несколько проще - подсчет прямого прохода копируется почти дословно.\n",
        "\n",
        "Обратный проход мы уже умеем - нужно просто вызвать `loss.backward()`.\n",
        "\n",
        "Для обновления `w` и `b` нужно иметь в виду следующее. Во-первых, pytorch не даст просто так обновить их:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx4DoGeBMJd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = torch.randn(1, requires_grad=True)\n",
        "\n",
        "w -= 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OjoUh-SMPBt",
        "colab_type": "text"
      },
      "source": [
        "Проблема в сложности поддержки in-place операций для работы autograd ([In place operations with autograd](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd)).\n",
        "\n",
        "Но нам и не нужна поддержка градиентов! Мы не будем делать backward pass через эту операцию - нужно всего лишь обновить значение переменной. Чтобы сделать это, можно воспользовать контекстом `no_grad`, либо производить обновление непосредственно буфера, который использует данный тензор:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zegkKd-cMOMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w.data -= 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVlaIdvHNXR_",
        "colab_type": "text"
      },
      "source": [
        "Другое, что нужно помнить - градиенты в тензорах накапливаются. Между вызовами `loss.backward()` нужно обнулять градиенты у `w` и `b`:\n",
        "```python\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "```\n",
        "\n",
        "**Задание** Реализовать линейную регрессию на pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRqxypuEU2ig",
        "colab_type": "code",
        "outputId": "8562a614-56fb-462d-9f9b-693ee30f3b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "X = torch.as_tensor(X).float()\n",
        "y = torch.as_tensor(y).float()\n",
        "\n",
        "w = torch.randn(1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "alpha = 0.1\n",
        "\n",
        "for i in range(100):\n",
        "    y_pred = w * X + b\n",
        "\n",
        "    loss = ((y-y_pred)**2).sum()/len(y_pred)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    w.data -= alpha * w.grad\n",
        "    b.data -= alpha * b.grad\n",
        "    \n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "    \n",
        "    if (i + 1) % 5 == 0:\n",
        "        display_progress(i + 1, loss, w.item(), b.item(), \n",
        "                         X.data.numpy(), y.data.numpy(), y_pred.data.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch = 100, Loss = 0.6325293183326721, w = 2.5778634548187256, b = -0.3179955780506134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAcNUlEQVR4nO3df5RU5Z3n8fe3qmkMiYGOuv4AocUf\nYfwxJlSvks0aojJGV2YcNWYIJJngGObkdGbiGWcyGg32umPG3R2TzBnJRodociY0xAiMHieOqKvk\n5KytdhExoOAioaHRKGpBskHoH/XsH9UFVbduVVd11a1bdevz+kfq3ltVTx/P+fTT3+d7n2vOOURE\nJJpiYQ9ARESCo5AXEYkwhbyISIQp5EVEIkwhLyISYW1hDyDX8ccf7zo7O8MehohIU0kmk287507w\nO9dQId/Z2Ul/f3/YwxARaSpmNlDsnMo1IiIRppAXEYkwhbyISIQp5EVEIkwhLyISYQp5EZEIU8iL\niIQsOZBixdM7SA6kav7ZDdUnLyLSapIDKZas7GNoJE17W4xVN8wjMaujZp+vmbyISIj6dr7D0Eia\ntIPhkTR9O9+p6ecr5EVEQjRv9nG0t8WIG0xqizFv9nE1/XyVa0REQpSY1cGqG+bRt/Md5s0+rqal\nGlDIi4iELjGro+bhnqVyjYhIhCnkRUTC9vYOGH4vkI9WyIuIhOXdndAzFe5JwOY1gXyFavIiIvXm\nHKy6DnY8cfTYOVcH8lUKeRGRevrp1+D5e4++/uPvwUc+G9jXKeRFROrhN6/Dt37vyMtDbhJd6Qf4\nYcd/JhHg1yrkRUSC1jM17+XNw19izejFxC1zx2tQ7ZOghVcRkeD8n3sKAj65dBf/Grs0sDtcvTST\nFxGptZEh+LsT8o9d/zjMnEcCAr3D1UshLyJSS56Ze+bYgbyXQd7h6qWQFxGphd3Pwf2X5R/7+hvQ\nPiWc8YxRyIuIVMs7ez9jAXxubThj8VDIi4j4SA6kxq+br70BfvmT/GOe0kzYFPIiIh7jPq1p+D24\n86T8N31+PZx+SX0HWoaahLyZ3Q8sBN5yzp07duxDwI+BTmAX8BnnXO0fYCgiUmN+T2s6EvJlLKxC\nmX8J1EGt+uR/AFzuOXYz8JRz7kzgqbHXIiINz/dpTS8/XBjwX3+jaMAvWdnH3Ru2s2RlXyAP6C5X\nTWbyzrmfmVmn5/BVwCfH/v1D4Bngb2vxfSIiQSp4WtMDnfkXnDAHup8r+v5iz20NY2YfZE3+ROfc\nG2P//jVwot9FZrYMWAYwc+bMAIcjIq2ukhJKYlYHiXUXwcY9+SfKWFjN/iUwPJJmUluMjintpWv8\nAarLwqtzzpmZK3LuPuA+gK6uLt9rRESqNe5iaq73UvDfO/OPffbH8OHLj3xWqV8W3r8EStb4AxZk\nyL9pZic7594ws5OBtwL8LhGRkrxBu27ToH9Qj7OwWu4vC+9drbkz+6D3q8kVZMg/AvwpcNfYfx8O\n8LtERErKLaHE4zF+0r+HkbQ7GtSD/wJPfCP/Tbe+CZOOyTs0kVl5QY2/2WryZraazCLr8WY2CNxO\nJtwfNLM/AwaAz9Tiu0REcpVbZ88N2r3732PN87uPBHXBwuqU4+BrO30/x1tvL3dWXs/9anLVqrum\n2GNNLq3F54uI+Kmozp7j3FOm0t4WY1t8UcG5OaNrWPUn84o+yCPMWflE6I5XEWlalZROcn8hTG87\nwLb4l/POf2X4L3l0dB5xG78EE9asfCIU8iLStCopnWR/IeycvLjgXHLpLp5c2Uc8Xf+F0aCZc43T\ntdjV1eX6+/vDHoaINJFya/L7v38N0/Y8lX/wtregbXJFn9OIzCzpnOvyO6eZvIg0tbJKJz1TmZbz\ncvh9xzPpb1+r/HOakEJeRJpKRTPuIj3vk4IZWkNSyItI0yi7m+atbfDdC/OPXfVd+OiS+gy0gSjk\nRaThZWfve/e/N343TZlbAbcKhbyINLTc2XtbPEZbzBhNu8IumH++FPZ6Gje+8TbEW6k4U0ghLyIN\nLbcXfnQ0zaILZnLKtPfl1+Q1ey9KIS8iDc3bC3/N3BkK9woo5EWkofluI7C7D+7/VP6Fn/kXOPuP\nwhlkA1PIi0jDy+thL2P23sw3NtWaQl5EmsPfnQgjh/KPLX8XYvG8QxPdtCyqavUgbxGRQCR3vZuZ\nvXsDvudAQcBD8eertirN5EWkcfVMLdzyd5yF1Ynu9x5VCnkRaTxb1sFDS/MOLRu+ifMXLGZehc9X\nbeVSDSjkRSRkBYukPgurpx/uZVJbjE9OaZ/Q81VbmUJeREKTu0jqt887t++n9/k9/Kctb3DFuSeT\nOjhU8fNVW51CXkRqYiJti30732F4ZISdkz9XeLLnAMmBFHc8upWhkTQv7HqX5QvPUb29Qgp5Eana\nRNsWuzcm6J7sOZizsOrtlEkdHFK9vUIKeRGpWrG2xaJh/Ox34fFb8g7tuHQlZ1x0Xd4xv04Z1dsr\no5AXkap5w7ij1AJpkTtWz/D5XHXKVE8hLyJV84ax38w+8UBn4RvL2ExMM/fqKORFpCa8YZyd2U9p\nS9O9seCWJuaMrmHVQEoBHjCFvEiLqcfmXdmZvd/sffbhXtIO4qYWyHpQyIu0kLpt3vXvt5Do+27+\nsSUPkWzvon1ln1og60ghL9JCfGvltQ75ElsBJ0ALqXWmkBdpIYFu3lXmU5q0kFpfCnmRFhJIS+LQ\n7+CbpxQe12P4GoJCXqTF1HQmrWesNrzAQ97MdgG/BUaBEedcV9DfKSIB610Erz6Wf+z6DTDzwnDG\nI0XVayZ/sXPu7Tp9l4jUWF7b5QRvapJwqFwjIiVl2y63xRfBRs9JhXvDq8czXh2wwcySZrbMe9LM\nlplZv5n179u3rw7DEZFK/GL7a5mA90gu3cWKp3eQHEiFMCoplznngv0Cs+nOub1m9h+AJ4C/cM79\nzO/arq4u19/fH+h4RKQCPguryaW7AOpzU5WUxcySxdY7A5/JO+f2jv33LWA9cEHQ3ykiVbr3EwUB\nf8+ZK0ku3UViVkfRrYWzkgMpzfIbRKA1eTN7PxBzzv127N+XAXcE+Z0iUiWf2fvsw720vRLj9SmD\nQOmbquq2dYKUJeiF1xOB9WaW/a5e59y/B/ydIuJj3I3JfMJ9xfwkd2/YTtrB0Eia1c/tZt2mQVbd\nMK/oTVV12TpByhZoyDvndgLnB/kdIjK+krPrA3vh22fnv8HicPu7zBtI0d4W4/BwGkemi+LwcJq1\nmwb55tXn+YZ3oFsnSMXUQinSAorOrse5YzW7DcK9G19jw8tvApmg/0n/Hq6dO8M35PU0p8aikBdp\nAd7Z9bLnLoON+Yul/OWL8KHTCt6bmNXB+adO44mX3yTbizcy6kqWYbQJWeNQyIu0gNzZdffGBBzy\nXDDOTU3zZh/HpLgxNJqJeZVhmodCXiTCvNsRFDyEr8w7VhOzOli97GOs3TSIAdcUKdVI41HIi0SE\nt3smu9h6yuheuttvyr+4oxO+urmiz1cJpjkp5EUiwNs9s3zhOTy25Y3MdgRxz8U9BzK/EJ7eoYXR\nFqCQF4mA3O6ZoZE0ix87j8WeazYveoHz55ylm5VaTD02KBORCSp3e4Bs90zcYOdkb7xn9ps5f85Z\ngH87pUSXZvIiDSS3rg6VbQLmV5qZM7qm4H26Wam1KORFGoS3jHLt3BllbQ/w6rOPknh8Sd6xw8fO\nZOXc9awaC/AVOfV33azUWhTyIg3CW0ZxMP6Mu2cqZ3kOrZifpPviM+im+HYG6pRpHQp5kRobdyOw\nIrxllGvnzuDauTP8P8tnO4LEoe/xu0nTjszeQZuFiUJepKaq6VwpVkYpeH+RrYA/fubx3LjgLNXf\nJY9CXqSGqp05lyyj+IR756FeIFPW8QZ89vNUf29tCnmRGgpk5rz5x7A+//HIv5l8Mh/5zd0AGPDp\nRPFtBlR/b20KeZEaqvnMuchWwP93IEX7yr68+r3XRNcGJFoCf5B3JfQgb5ExfuF+y16Y/IEjL0uF\nuO5qbS2lHuStmbxIoxnnQR5Zpcow6qqRLIW8SKMoM9zLoa4ayVLIi4Ttmbvgmb/PPzY9AV/63xP+\nSHXVSJZCXiRMNZy9e6mrRkAhL1KRajpWvE9pKnDbPmhrr81ARcYo5EXKVE3HytH3jrJz8pLCCzyz\nd7U/Sq0o5EXKVE3HytpNg0Wf0pSVDfaOKe3c8ehWtT9KTSjkRcrk7VjpmNKet4VvMftWf5lvbu/N\nO7b/1AVM+7O1R17n/pUQM2M07XCo/VGqp5AXKVNux0p2tn14OE08Ztxx1bksvnBm4Zt6pnKC59Ct\n5/+cO68+L+9Y7l8J4IjHDOec2h+lagp5kQpkO1ZWPL2Dw8OZPd9H0o7lD2/hwycde3TG7dM1c/rh\nHzGprY1VPlsQeP9KWL7wHFIHh1STl6op5EUmYN7s44jHjJHM1Ju0c5myyqkfhDs+VHD9aYd6iceM\n5QvP8Q1t9bVLULR3jcgE9T63m+UPbyHtHO1tsczCqseK+Unu3rCdtIO4wZ9cMJPp096nIJeaCnXv\nGjO7HPhHMn0FK51zdwX9nSL1sPjCmXz4pGM5cf11zNj/Qv7JxFL4w+8wbyB1pAwTjxkPJQcZGVXX\njNRPoCFvZnFgBfAHwCDwgpk94px7OcjvFQmKt3/d96amnLbI3DLM6/vfY/Xzu7VpmNRV0DP5C4Ad\nzrmdAGa2BrgKUMhL08ltc9w5eXHB+RXzk5nw9xzPLtYmB1Ks3TSoTcOkroIO+enAnpzXg8CFuReY\n2TJgGcDMmT4taCINom/nOzBymJ2T/7Tg3JzRNQxt2F6yDKPFVQlDLOwBOOfuc851Oee6TjjB21Es\n0ji6NybY5g34ngOsmJ8suBM2KzmQYsXTO0gOpIBM0HdffIYCXuom6Jn8XuDUnNczxo6JNJyi+8Xc\n90l4/Rd51+5N/A3T//A2oPje7Xo6kzSCoEP+BeBMMzuNTLgvAgqLmSIhSw6k+Ox9zzI86pgUN1Yv\n+1gmkItsBTw952WxMoyeziSNINCQd86NmNlXgMfJtFDe75zbGuR3ilQiO3vfvGc/Q6OZe0aGRt24\nXTNefnu36+lM0ggC75N3zv0U+GnQ3yNSqdxyio0d+wAH2XLMDQXXzhldw6qBVEUzcS20SiPQtgbS\nsnLLKTFg1zGFlcTZh3vH7ladWLlFT2eSsCnkpWVlyym/jC1hko3mn7z6PpLTLqN9ZZ/KLdLUFPLS\nshKzOnz3m8nW3hOgcos0PYW8tKYyH6Ctcos0u9BvhhKpq9++WXbAi0SBZvLSOhTu0oIU8hJ9fuH+\n+fVw+iX1H4tInSnkJdo0e5cWp5CXaPIJ9+TSXVpElZajhVeJhOxuj1s2v1DkIdq9JXeHFIkqzeSl\nKeXuGAmwZGWfb8/7nNE12h1SWppCXpqON6S3xRexLZ5/zbYr1/HU/5vF8intpA4OaXdIaVkKeWk6\nuSHtN3vvveKX3PHIVoZG/J/UpN0hpZUo5KXpzJt9XMlnrKbGmalrd0hpJQp5aRrJgRS/evEZPv2L\nLxae7DlAd87L8Wbq2q5AWoVCXppCciBF4oFOEt4TRfab0UxdJEMhL42vZ2pBuK/6j2tZcuWCom/R\nTF0kQ33yEoqy+9R9et7njK5hzrkFc3oR8aGZvNRdWX3qRe5Y7dv5DqtUghEpm0Je6q5kn/qOp+BH\n1xS+qecACVC4i1RIIS91V7RPXZuJidScQl7qrqD75YHOgms+Nvy/uGfZFYXdNCJSEYW8hOJI94vP\n7L3zUC8G2m5ApAYU8hKo3I3E8gLbJ9zPGlnD0EgagElx03YDIjWgkJfA+HbRpB6Df/1y3nVDLs4v\nr3+N1cC6TYM44Nq5MzSLF6kBhbzUjHfW7u2i8au9dx7qJWZw08536L74DAW7SI0p5KUm/Gbt2S4a\nv50iH1zwLF979FcApB10TGmv95BFWoLueJWaKNb77hfw9Bxg33A7NvYyBqQODtVzuCItQzN5qQlv\n73v3xgRs9FyU0/M+b/ZxTJ6kPd1FgmbOuWA+2KwH+BKwb+zQ151zPy31nq6uLtff3x/IeCR4yYEU\nB3/2T1z02t35J075KCx7xvd67RQpUj0zSzrnuvzOBT2T/7Zz7h8C/g5pEH4Lq6XuWNVOkSLBU7lG\nque3HcFtb0Hb5PqPRUTyBL3w+hUze8nM7jcz3ymbmS0zs34z69+3b5/fJdLIiu03o4AXaQhV1eTN\n7EngJJ9TtwJ9wNuAA/4bcLJz7vpSn6eafBPRZmIiDSOwmrxzrvijefIH8M/Ao9V8l1SvJgudT/5X\n+Pm38o/9/iK45t7qB+hDi7Mi1QmsJm9mJzvn3hh7eTWwJajvkvGV9aCO8dR59l6TMYu0uCAXXv+H\nmX2ETLlmF/DnAX6XjKPkgzrG4xfuy1MQC3ZJp6oxiwgQYMg75z4f1GdL5Yo+qMPHkRLJaR0kfjC7\n8II61d4rGbOI+AvsZqiJ0MJrsMqpb2dLJMW2I6g31eRFxhfmzVDSQMq6+ejJHrbFf5B36IVZXyJ2\nya2hPKVJN0yJVEchL0f1TC0I8rOGVzPyqqPttT4+nZihfd5FmoxCXnwXVld8op+9Bw4x8vxu0g6G\nRtKsfm436zYNqstFpIloq+FWNjpctC2y+5IzuXbuDNrbYke2BHYc7XIRkeagmXyLyS5kdm/0qbB7\nFlYTszpYdcM81m0a5Cf9exhNO3W5iDQZhXwLSQ6k2Pb9ZXTHNuSfuPb7cN6nfd+TXfi8Zu4MdbmI\nNCGFfAtJPNBJwlugK7MtUl0uIs1JId8KfOruc0bXZBZQQxiOiNSPQj7Kht+DOws3CV0xP8kqlV1E\nWoJCPqpKbCbWXeQturtUJHoU8lGz7s/hpTX5x77wCMyeX/Jt2vFRJJoU8lFSxVbA2vFRJJoU8lFQ\ng33eteOjSDQp5JvZoQNw18zC4xPYLTJ745Nq8iLRopBvVj6z9+TSXVWFs3rhRaJHe9c0m9WLCwL+\nj0fvYvbhXpas7CM5kAppYCLSiBTyDSY5kGLF0zv8w7pnKmz/t7xDK+YneWlkZt6CqYhIlso1DaRo\nG2OJhdV5AyktmIpIUQr5BuJtY9y87VUSD1yWf9Ex0+DmgSMvtWAqIqUo5BtIbhvja5MXw7OeC4p0\nzWjBVESKUcg3kMSsDpIn/j3vf3tz/om/2ATHnR7OoESkqSnkG0nPVN5fcKzynncRkSyFfCOowR2r\nIiJ+1EIZpgODhQE/82MKeBGpGc3kK1Sz7Xg1exeROlDIV6Dc7XhL/iJ44EoY+Hn+sZtehWNPHP+9\nIiIVUshXoJzteEv+Ihhn9q493UWk1hTyFShnO17fXwQPdBZ+mE9pRnu6i0itVbXwambXmdlWM0ub\nWZfn3C1mtsPMtpvZp6obZmPI3l36V5d9uOgsO/uLIG5wRtubdG/0PCp77heK1t5z36stCkSkFsw5\nN/E3m/0ekAbuBf7aOdc/dvxsYDVwAXAK8CRwlnNutNTndXV1uf7+/gmPp1EkB1Jlz969NXjV5EWk\nUmaWdM51+Z2rqlzjnHtl7Au8p64C1jjnDgO/MrMdZALfe6N+9KxcQGLwhfxjN++BYz5YcGmxGrzC\nXURqJag++enAnpzXg2PHoq1nKngDvudA0YD/zpOvFtTgRURqadyZvJk9CZzkc+pW59zD1Q7AzJYB\nywBmzvR5lF0zqLDnPTuDPzycxgEx1eBFJCDjhrxzbsEEPncvcGrO6xljx/w+/z7gPsjU5CfwXeF5\n4yW496L8Y5fcBp/4m5Jvy3bRODJ/Sn38jOO5ccFZKtOISM0F1UL5CNBrZt8is/B6JvB8QN9VoJaL\nl0U/q4o7Vr2tmAp4EQlKVSFvZlcD/wScAPybmb3onPuUc26rmT0IvAyMAN3jddbUSjU3FPl1uhR8\n1o/OgeHf5b/x1l/DpPeVPUY96ENE6qXa7pr1wPoi5+4E7qzm8ydiojcU+QW697PKbYssh7poRKQe\nInfHazl3pfrx++WQ/axt8UUF1yeX7lJIi0jDi1zIT7QU4vfLIdG+uyDgvzH8RXrTl/FX2nJARJpA\n5EIeJlYKKfjl4FOamTO6huF0ZX8hiIiEKZIhP1GJWR0k1s+HjQP5J77xDsTbWFWia0fbEYhII1LI\n5xqnLbLYXwjaIlhEGlVLhPy4s+wqn9KkLYJFpFFFMuSzod4xpZ0trx/goeQgI6M+s+zBJKy8JP/N\nn1sLZ1R2k+9EO3pERIIWuZDPLZ2kPZsk5M2ya/iMVd3cJCKNKnIhn1s6yWVkNgH7wvZu2NiXf/L2\n/eDZLrnShVTd3CQijShyIZ8tnWSDPga0xY3rEjO486WL4Nc5Fx97Mty0reAztJAqIlERuZDPLZ10\nTGkndXAo8wi+lzwXlijNaCFVRKIiciEPOaWT1C74x/xnrF4xdBe/ineyaiBVNLi1kCoiURHJkAd8\nF1ZnH+4l7SDuSs/OtZAqIlERvZDfvxu+c17+sdv30/v8HmIPbwHnypqdayFVRKIgOiHvHDz4BXjl\nkaPHFvXCnCtJDqS449GtpJ0jFjOWLzxHAS4iLSEaIT90EL558tHXC78NXdcfeZm7kOqc45ntb5E6\nOKRSjIhEXiRCfssrWzkXOPyBU5l8YxLaJh85lxxIsXf/e8RiRnrU4YANL7/JEy+/yeRJao8UkWhr\n+pBPDqRY8tDbDI300j4aY9XegyRmTT56bqzf3Tzvc6g9UkSiLxb2AKrl19Pud845aIsZsbG0j4Ha\nI0Uk8pp+Jl+qp917bvnCc0gdHDpyk5Rq8iISdeacG/+qOunq6nL9/f0Vv6/UPjN6mIeIRJ2ZJZ1z\nXX7nmn4mD6V72tXvLiKtrOlr8iIiUpxCXkQkwhTyIiIRppAXEYkwhbyISIQp5EVEIqyh+uTNbB8w\nEPY4ynQ88HbYgwiJfvbW1Mo/OzT2zz/LOXeC34mGCvlmYmb9xW4+iDr97PrZW1Gz/vwq14iIRJhC\nXkQkwhTyE3df2AMIkX721tTKPzs06c+vmryISIRpJi8iEmEKeRGRCFPIV8nMbjIzZ2bHhz2WejKz\n/2lm28zsJTNbb2bTwh5T0MzscjPbbmY7zOzmsMdTL2Z2qpk9bWYvm9lWM/tq2GOqNzOLm9kvzOzR\nsMdSKYV8FczsVOAyYHfYYwnBE8C5zrnfB14Fbgl5PIEysziwArgCOBv4rJmdHe6o6mYEuMk5dzYw\nD+huoZ8966vAK2EPYiIU8tX5NvA1Ms8FbynOuQ3OuZGxl33AjDDHUwcXADucczudc0PAGuCqkMdU\nF865N5xzm8b+/VsyYTc93FHVj5nNAK4EVoY9lolQyE+QmV0F7HXObQ57LA3geuCxsAcRsOnAnpzX\ng7RQ0GWZWSfwUeC5cEdSV98hM5lLhz2QiYjE4/+CYmZPAif5nLoV+DqZUk1klfr5nXMPj11zK5k/\n51fVc2xSf2b2AWAtcKNz7jdhj6cezGwh8JZzLmlmnwx7PBOhkC/BObfA77iZnQecBmw2M8iUKjaZ\n2QXOuV/XcYiBKvbzZ5nZF4GFwKUu+jdc7AVOzXk9Y+xYSzCzSWQCfpVzbl3Y46mjjwN/ZGb/BTgG\n+KCZ/cg597mQx1U23QxVA2a2C+hyzjXqDnU1Z2aXA98C5jvn9oU9nqCZWRuZBeZLyYT7C8Bi59zW\nUAdWB5aZyfwQeNc5d2PY4wnL2Ez+r51zC8MeSyVUk5eJugc4FnjCzF40s++FPaAgjS0yfwV4nMzC\n44OtEPBjPg58Hrhk7P/1i2MzW2kCmsmLiESYZvIiIhGmkBcRiTCFvIhIhCnkRUQiTCEvIhJhCnkR\nkQhTyIuIRNj/B15wiRJqQPd+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaKTKN_fOvo-",
        "colab_type": "text"
      },
      "source": [
        "Думать нужно уже гораздо меньше, да? :)\n",
        "\n",
        "Про другие фишки низкоуровнего pytorch можно почитать здесь: [PyTorch — ваш новый фреймворк глубокого обучения](https://habr.com/post/334380/) (статья веселая, но немного устарела, читать лучше с оглядкой на [PyTorch 0.4.0 Migration Guide](https://pytorch.org/blog/pytorch-0_4_0-migration-guide/))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZNq6ujzPtvd",
        "colab_type": "text"
      },
      "source": [
        "## Word embeddings и высокоуровневый API PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITLgcVz66AfV",
        "colab_type": "text"
      },
      "source": [
        "Займёмся рассмотрением высокоуровневого API - в нем уже реализованы разные классы-запчасти для обучения нейронок.\n",
        "\n",
        "Будем решать всё ту же задачу, что и в прошлый раз - обучение словных эмбеддингов, только теперь мы будем учить их самостоятельно!\n",
        "\n",
        "Для начала нужно подготовить данные для обучения.\n",
        "\n",
        "Соберем и токенизируем тексты:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKKb9Ya8hzIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
        "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
        "\n",
        "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYoj91iDDDfT",
        "colab_type": "text"
      },
      "source": [
        "Соберем индекс самых частотных слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PL471pGjuVN",
        "colab_type": "code",
        "outputId": "9ffe24d7-50e2-4f56-e62c-1d0ad9663901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "MIN_COUNT = 5\n",
        "\n",
        "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
        "word2index = {\n",
        "    '<unk>': 0\n",
        "}\n",
        "\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < MIN_COUNT:\n",
        "        break\n",
        "        \n",
        "    word2index[word] = len(word2index)\n",
        "    \n",
        "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]\n",
        "    \n",
        "print('Vocabulary size:', len(word2index))\n",
        "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
        "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
        "print('Most freq words:', index2word[1:21])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 28634\n",
            "Tokens count: 6969946\n",
            "Unknown tokens appeared: 123601\n",
            "Most freq words: ['?', 'the', 'what', 'is', 'a', 'i', 'to', 'in', 'how', 'of', 'do', 'are', 'and', 'for', ',', 'can', 'you', 'why', 'it', 'my']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF5mYpCsE9Uh",
        "colab_type": "text"
      },
      "source": [
        "### Skip-Gram Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om1IG5XEMGRa",
        "colab_type": "text"
      },
      "source": [
        "Начнем с skip-gram модели обучения word2vec.\n",
        "\n",
        "Это простая модель всего из двух слоев. Ее идея - учить вектора эмбеддингов такими, чтобы по ним можно было как можно лучше предсказать контекст соответствующих слов. То есть если мы хорошо научились кодировать слова, с которыми встречается данное - значит, мы что-то знаем и о нем самом. Например, естественным образом получится, что слова, встречающиеся в одинаковых контекстах (скажем, `apple` и `orange`)  будут иметь близкие вектора эмбеддингов.\n",
        "\n",
        "![](https://ask.qcloudimg.com/http-save/yehe-1565119/pv4604cabp.jpeg)  \n",
        "*From cs224n, Lecture 2*\n",
        "\n",
        "Для этого мы моделируем вероятности $\\{P(w_{c+j}|w_c):  j = c-k, ..., c+k, j \\neq c\\}$, где $k$ - размер контекстного окна, $c$ - индекс центрального слова.\n",
        "\n",
        "Соберем такую модель: будем учить пару матриц $U$ - матрицу эмбеддингов, которую потом и возьмем для своих задач, и $V$ - матрицу выходного слоя.\n",
        "\n",
        "Каждому слову в словаре соответствует строка в матрице $U$ и столбец $V$.\n",
        "\n",
        "![skip-gram](https://image.ibb.co/khFXu9/Skip_gram.png)\n",
        "\n",
        "Что тут происходит? Слово отображается в эмбеддинг - строку $u_c$. Дальше этот эмбеддинг умножается на матрицу $V$. \n",
        "\n",
        "В итоге получаем набор числе $v_j^T u_c$ - степень похожести слова с номером $j$ и нашего слова.\n",
        "\n",
        "Преобразуем эти числа в что-то вроде вероятностей - воспользуемся функцией softmax: $P(i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$.\n",
        "\n",
        "А дальше будем считать кросс-энтропийные потери:\n",
        "\n",
        "$$-\\sum_{-k \\leq j \\leq k, j \\neq 0} \\log \\frac{\\exp(v_{c+j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)} \\to \\min_{U, V}.$$\n",
        "\n",
        "В итоге, вектор $u_c$ будет приближаться к векторам $v_{c_j}$ из его контекста.\n",
        "\n",
        "Реализуем это всё, чтобы разобраться.\n",
        "\n",
        "#### Генерация батчей\n",
        "\n",
        "Для начала нужно собрать контексты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocrsXgaynYPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_contexts(tokenized_texts, window_size):\n",
        "    contexts = []\n",
        "    for tokens in tokenized_texts:\n",
        "        for i in range(len(tokens)):\n",
        "            central_word = tokens[i]\n",
        "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1) \n",
        "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
        "\n",
        "            contexts.append((central_word, context))\n",
        "            \n",
        "    return contexts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQBa6yQ9BXjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contexts = build_contexts(tokenized_texts, window_size=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyQNK-9SBdb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contexts[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkmV-xQDyhRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_texts[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbQKln_6yC4l",
        "colab_type": "text"
      },
      "source": [
        "Преобразуем слова в индексы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOPRlKlLvUBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context]) \n",
        "            for central_word, context in contexts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYmrAi9gyIe-",
        "colab_type": "text"
      },
      "source": [
        "Реализуем генератор батчей для нашей нейронки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6opX5cEp8LxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def make_skip_gram_batchs_iter(contexts, window_size, num_skips, batch_size):\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * window_size\n",
        "    \n",
        "    central_words = [word for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "    contexts = [context for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "    \n",
        "    batch_size = int(batch_size / num_skips)\n",
        "    batchs_count = int(math.ceil(len(contexts) / batch_size))\n",
        "    \n",
        "    print('Initializing batchs generator with {} batchs per epoch'.format(batchs_count))\n",
        "    \n",
        "    while True:\n",
        "        indices = np.arange(len(contexts))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(batchs_count):\n",
        "            batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
        "            batch_indices = indices[batch_begin: batch_end]\n",
        "\n",
        "            batch_data, batch_labels = [], []\n",
        "\n",
        "            for data_ind in batch_indices:\n",
        "                central_word, context = central_words[data_ind], contexts[data_ind]\n",
        "                \n",
        "                words_to_use = random.sample(context, num_skips)\n",
        "                batch_data.extend(words_to_use)\n",
        "                batch_labels.extend([central_word] * num_skips)\n",
        "            \n",
        "            yield batch_data, batch_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0D79MwB_gMe",
        "colab_type": "code",
        "outputId": "7ea67b11-206e-4c02-8222-c8c19699cb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "batch, labels = next(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=2, batch_size=32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing batchs generator with 295262 batchs per epoch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DXjZS3JyQZh",
        "colab_type": "text"
      },
      "source": [
        "#### nn.Sequential\n",
        "\n",
        "Простейший способ реализовать модель на PyTorch - использовать модуль `nn.Sequential`. В нем нужно просто перечислить все слои, и он будет применять их последовательно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRw9Z4G__46O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Embedding(len(word2index), 32),\n",
        "    nn.Linear(32, len(word2index))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysn0DDpLyj1S",
        "colab_type": "text"
      },
      "source": [
        "Еще одна особенность pytorch, о которой до сих пор не говорили - поддержка вычислений на видеокарте. На видеокарте большинство нейронок считается гораздо быстрее благодаря высокой параллелизации. Сказать pytorch'у, чтобы он считал на видеокарте, очень просто:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfmaUi3Uy9YT",
        "colab_type": "code",
        "outputId": "a2bdfc59-7a51-49d6-ea07-01dacfed770d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Embedding(28634, 32)\n",
              "  (1): Linear(in_features=32, out_features=28634, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3c3UEa2zHhk",
        "colab_type": "text"
      },
      "source": [
        "либо"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHxAg5ZWzEKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHi1CL2pzxOg",
        "colab_type": "text"
      },
      "source": [
        "Создать тензоры на видеокарте можно, например, так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycx1O3_SzvmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = torch.cuda.LongTensor(batch)\n",
        "labels = torch.cuda.LongTensor(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtLMvOO2z3c8",
        "colab_type": "text"
      },
      "source": [
        "Заставить модель посчитать значение можно так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9wTpewTz3Dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = model(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWJmDy_uzJgD",
        "colab_type": "text"
      },
      "source": [
        "Теперь нам нужна функция потерь"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7rlD62_ykYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = nn.CrossEntropyLoss().cuda() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxLBiBua0OZM",
        "colab_type": "text"
      },
      "source": [
        "Посчитать значение можно так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCaTB5cc0GVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = loss_function(logits, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAwx-pck0RxX",
        "colab_type": "text"
      },
      "source": [
        "А теперь, конечно же, backprop!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWt6gL0_0Npp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJkDOl6szRLm",
        "colab_type": "text"
      },
      "source": [
        "И, наконец, оптимизатор.\n",
        "\n",
        "Будем использовать Adam. Интерфейс - передать список оптимизируемых параметров и learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-b5CIARzQ6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.01) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju5lO0Xi0hsV",
        "colab_type": "text"
      },
      "source": [
        "Оптимизация идет просто - нужно вызвать `step()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9QK7nHu0Zw8",
        "colab_type": "code",
        "outputId": "9536d611-e0a1-42b1-dc9e-db0e38521bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "print(model[0].weight)\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(model[0].weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.6786, -0.2872, -0.6197,  ...,  0.4792, -0.5476,  0.0551],\n",
            "        [ 0.1715,  0.5830,  1.1244,  ...,  1.1589,  0.6733, -1.0446],\n",
            "        [-1.2741,  1.1838, -0.1995,  ...,  1.0459, -0.7306, -1.2047],\n",
            "        ...,\n",
            "        [ 0.6380,  0.5332,  1.3211,  ..., -0.9986, -0.9841, -1.2183],\n",
            "        [-0.4942, -0.0171,  0.9920,  ...,  0.2858, -0.0403, -0.3376],\n",
            "        [ 0.8804, -1.0016,  1.3574,  ..., -1.5975, -0.9484,  0.6068]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.6786, -0.2872, -0.6197,  ...,  0.4792, -0.5476,  0.0551],\n",
            "        [ 0.1815,  0.5930,  1.1344,  ...,  1.1489,  0.6633, -1.0346],\n",
            "        [-1.2841,  1.1738, -0.1895,  ...,  1.0559, -0.7406, -1.2147],\n",
            "        ...,\n",
            "        [ 0.6380,  0.5332,  1.3211,  ..., -0.9986, -0.9841, -1.2183],\n",
            "        [-0.4942, -0.0171,  0.9920,  ...,  0.2858, -0.0403, -0.3376],\n",
            "        [ 0.8804, -1.0016,  1.3574,  ..., -1.5975, -0.9484,  0.6068]],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnxyk1ew0pSk",
        "colab_type": "text"
      },
      "source": [
        "И последнее - нужно обнулить градиенты!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMsuvEP90svi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PibTw33Azg7q",
        "colab_type": "text"
      },
      "source": [
        "#### Реализация обучения skip-gram модели\n",
        "\n",
        "Наконец, напишем цикл обучения - как уже было с линейной регрессией.\n",
        "\n",
        " **Задание** Заполните цикл."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewGMgYTXANzz",
        "colab_type": "code",
        "outputId": "fcbfbe24-b477-413d-8e1e-216f8da7ab95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, (batch, labels) in enumerate(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=4, batch_size=128)):\n",
        "    #<1. convert data to tensors>\n",
        "\n",
        "    batch = torch.cuda.LongTensor(batch)\n",
        "    labels = torch.cuda.LongTensor(labels)\n",
        "     \n",
        "    #<2. make forward pass>\n",
        "\n",
        "    batch_res = model(batch)\n",
        "    logits = F.log_softmax(batch_res)\n",
        "\n",
        "    #<3. make backward pass>\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss().cuda() \n",
        "    loss = loss_function(logits, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    #<4. apply optimizer>\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
        "    optimizer.step()\n",
        "\n",
        "    #<5. zero grads>\n",
        "    optimizer.zero_grad()\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing batchs generator with 147631 batchs per epoch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step = 1000, Avg Loss = 8.7459, Time = 7.84s\n",
            "Step = 2000, Avg Loss = 8.1727, Time = 5.60s\n",
            "Step = 3000, Avg Loss = 8.5347, Time = 5.60s\n",
            "Step = 4000, Avg Loss = 9.0414, Time = 5.59s\n",
            "Step = 5000, Avg Loss = 9.3803, Time = 5.61s\n",
            "Step = 6000, Avg Loss = 9.5823, Time = 5.59s\n",
            "Step = 7000, Avg Loss = 9.6416, Time = 5.60s\n",
            "Step = 8000, Avg Loss = 9.6975, Time = 5.60s\n",
            "Step = 9000, Avg Loss = 9.6996, Time = 5.60s\n",
            "Step = 10000, Avg Loss = 9.7184, Time = 5.59s\n",
            "Step = 11000, Avg Loss = 9.7476, Time = 5.60s\n",
            "Step = 12000, Avg Loss = 9.7126, Time = 5.60s\n",
            "Step = 13000, Avg Loss = 9.7284, Time = 5.61s\n",
            "Step = 14000, Avg Loss = 9.7438, Time = 5.60s\n",
            "Step = 15000, Avg Loss = 9.7385, Time = 5.59s\n",
            "Step = 16000, Avg Loss = 9.7274, Time = 5.61s\n",
            "Step = 17000, Avg Loss = 9.7396, Time = 5.59s\n",
            "Step = 18000, Avg Loss = 9.7437, Time = 5.60s\n",
            "Step = 19000, Avg Loss = 9.7732, Time = 5.59s\n",
            "Step = 20000, Avg Loss = 9.7395, Time = 5.60s\n",
            "Step = 21000, Avg Loss = 9.7747, Time = 5.60s\n",
            "Step = 22000, Avg Loss = 9.7782, Time = 5.60s\n",
            "Step = 23000, Avg Loss = 9.8276, Time = 5.60s\n",
            "Step = 24000, Avg Loss = 9.7567, Time = 5.60s\n",
            "Step = 25000, Avg Loss = 9.7328, Time = 5.60s\n",
            "Step = 26000, Avg Loss = 9.7924, Time = 5.62s\n",
            "Step = 27000, Avg Loss = 9.7703, Time = 5.60s\n",
            "Step = 28000, Avg Loss = 9.7540, Time = 5.60s\n",
            "Step = 29000, Avg Loss = 9.8088, Time = 5.60s\n",
            "Step = 30000, Avg Loss = 9.7490, Time = 5.61s\n",
            "Step = 31000, Avg Loss = 9.7365, Time = 5.60s\n",
            "Step = 32000, Avg Loss = 9.7036, Time = 5.61s\n",
            "Step = 33000, Avg Loss = 9.7393, Time = 5.60s\n",
            "Step = 34000, Avg Loss = 9.7323, Time = 5.60s\n",
            "Step = 35000, Avg Loss = 9.7930, Time = 5.61s\n",
            "Step = 36000, Avg Loss = 9.7527, Time = 5.61s\n",
            "Step = 37000, Avg Loss = 9.7861, Time = 5.62s\n",
            "Step = 38000, Avg Loss = 9.7848, Time = 5.60s\n",
            "Step = 39000, Avg Loss = 9.7557, Time = 5.60s\n",
            "Step = 40000, Avg Loss = 9.7269, Time = 5.60s\n",
            "Step = 41000, Avg Loss = 9.7793, Time = 5.60s\n",
            "Step = 42000, Avg Loss = 9.7675, Time = 5.60s\n",
            "Step = 43000, Avg Loss = 9.7725, Time = 5.61s\n",
            "Step = 44000, Avg Loss = 9.8059, Time = 5.60s\n",
            "Step = 45000, Avg Loss = 9.7374, Time = 5.60s\n",
            "Step = 46000, Avg Loss = 9.7371, Time = 5.60s\n",
            "Step = 47000, Avg Loss = 9.7494, Time = 5.60s\n",
            "Step = 48000, Avg Loss = 9.7459, Time = 5.61s\n",
            "Step = 49000, Avg Loss = 9.7601, Time = 5.58s\n",
            "Step = 50000, Avg Loss = 9.7392, Time = 5.59s\n",
            "Step = 51000, Avg Loss = 9.7788, Time = 5.58s\n",
            "Step = 52000, Avg Loss = 9.8092, Time = 5.59s\n",
            "Step = 53000, Avg Loss = 9.8049, Time = 5.60s\n",
            "Step = 54000, Avg Loss = 9.7445, Time = 5.59s\n",
            "Step = 55000, Avg Loss = 9.7306, Time = 5.59s\n",
            "Step = 56000, Avg Loss = 9.7757, Time = 5.60s\n",
            "Step = 57000, Avg Loss = 9.7464, Time = 5.60s\n",
            "Step = 58000, Avg Loss = 9.7282, Time = 5.60s\n",
            "Step = 59000, Avg Loss = 9.7423, Time = 5.60s\n",
            "Step = 60000, Avg Loss = 9.7576, Time = 5.68s\n",
            "Step = 61000, Avg Loss = 9.7375, Time = 5.59s\n",
            "Step = 62000, Avg Loss = 9.7915, Time = 5.59s\n",
            "Step = 63000, Avg Loss = 9.7271, Time = 5.57s\n",
            "Step = 64000, Avg Loss = 9.6920, Time = 5.58s\n",
            "Step = 65000, Avg Loss = 9.7327, Time = 5.58s\n",
            "Step = 66000, Avg Loss = 9.7214, Time = 5.58s\n",
            "Step = 67000, Avg Loss = 9.7746, Time = 5.59s\n",
            "Step = 68000, Avg Loss = 9.7518, Time = 5.59s\n",
            "Step = 69000, Avg Loss = 9.7862, Time = 5.60s\n",
            "Step = 70000, Avg Loss = 9.7536, Time = 5.59s\n",
            "Step = 71000, Avg Loss = 9.7666, Time = 5.59s\n",
            "Step = 72000, Avg Loss = 9.7635, Time = 5.59s\n",
            "Step = 73000, Avg Loss = 9.7272, Time = 5.60s\n",
            "Step = 74000, Avg Loss = 9.7188, Time = 5.58s\n",
            "Step = 75000, Avg Loss = 9.7195, Time = 5.60s\n",
            "Step = 76000, Avg Loss = 9.7333, Time = 5.58s\n",
            "Step = 77000, Avg Loss = 9.7513, Time = 5.59s\n",
            "Step = 78000, Avg Loss = 9.7678, Time = 5.59s\n",
            "Step = 79000, Avg Loss = 9.7411, Time = 5.59s\n",
            "Step = 80000, Avg Loss = 9.7637, Time = 5.60s\n",
            "Step = 81000, Avg Loss = 9.7234, Time = 5.60s\n",
            "Step = 82000, Avg Loss = 9.7479, Time = 5.59s\n",
            "Step = 83000, Avg Loss = 9.7427, Time = 5.59s\n",
            "Step = 84000, Avg Loss = 9.7359, Time = 5.59s\n",
            "Step = 85000, Avg Loss = 9.7044, Time = 5.59s\n",
            "Step = 86000, Avg Loss = 9.7386, Time = 5.59s\n",
            "Step = 87000, Avg Loss = 9.7556, Time = 5.59s\n",
            "Step = 88000, Avg Loss = 9.6852, Time = 5.59s\n",
            "Step = 89000, Avg Loss = 9.7868, Time = 5.59s\n",
            "Step = 90000, Avg Loss = 9.7210, Time = 5.59s\n",
            "Step = 91000, Avg Loss = 9.7248, Time = 5.59s\n",
            "Step = 92000, Avg Loss = 9.7506, Time = 5.59s\n",
            "Step = 93000, Avg Loss = 9.7598, Time = 5.59s\n",
            "Step = 94000, Avg Loss = 9.6976, Time = 5.59s\n",
            "Step = 95000, Avg Loss = 9.7508, Time = 5.58s\n",
            "Step = 96000, Avg Loss = 9.6982, Time = 5.59s\n",
            "Step = 97000, Avg Loss = 9.7786, Time = 5.58s\n",
            "Step = 98000, Avg Loss = 9.7600, Time = 5.59s\n",
            "Step = 99000, Avg Loss = 9.6765, Time = 5.59s\n",
            "Step = 100000, Avg Loss = 9.7174, Time = 5.60s\n",
            "Step = 101000, Avg Loss = 9.7877, Time = 5.59s\n",
            "Step = 102000, Avg Loss = 9.8142, Time = 5.60s\n",
            "Step = 103000, Avg Loss = 9.7609, Time = 5.59s\n",
            "Step = 104000, Avg Loss = 9.8166, Time = 5.60s\n",
            "Step = 105000, Avg Loss = 9.7701, Time = 5.59s\n",
            "Step = 106000, Avg Loss = 9.7628, Time = 5.59s\n",
            "Step = 107000, Avg Loss = 9.7319, Time = 5.59s\n",
            "Step = 108000, Avg Loss = 9.7298, Time = 5.60s\n",
            "Step = 109000, Avg Loss = 9.7777, Time = 5.58s\n",
            "Step = 110000, Avg Loss = 9.8136, Time = 5.59s\n",
            "Step = 111000, Avg Loss = 9.7390, Time = 5.58s\n",
            "Step = 112000, Avg Loss = 9.7148, Time = 5.59s\n",
            "Step = 113000, Avg Loss = 9.7152, Time = 5.60s\n",
            "Step = 114000, Avg Loss = 9.7461, Time = 5.60s\n",
            "Step = 115000, Avg Loss = 9.7490, Time = 5.59s\n",
            "Step = 116000, Avg Loss = 9.7535, Time = 5.59s\n",
            "Step = 117000, Avg Loss = 9.7290, Time = 5.59s\n",
            "Step = 118000, Avg Loss = 9.7625, Time = 5.59s\n",
            "Step = 119000, Avg Loss = 9.7667, Time = 5.60s\n",
            "Step = 120000, Avg Loss = 9.7453, Time = 5.59s\n",
            "Step = 121000, Avg Loss = 9.7914, Time = 5.59s\n",
            "Step = 122000, Avg Loss = 9.7571, Time = 5.60s\n",
            "Step = 123000, Avg Loss = 9.7584, Time = 5.59s\n",
            "Step = 124000, Avg Loss = 9.6863, Time = 5.60s\n",
            "Step = 125000, Avg Loss = 9.7442, Time = 5.60s\n",
            "Step = 126000, Avg Loss = 9.7059, Time = 5.59s\n",
            "Step = 127000, Avg Loss = 9.7747, Time = 5.59s\n",
            "Step = 128000, Avg Loss = 9.7759, Time = 5.58s\n",
            "Step = 129000, Avg Loss = 9.7824, Time = 5.59s\n",
            "Step = 130000, Avg Loss = 9.7463, Time = 5.59s\n",
            "Step = 131000, Avg Loss = 9.7296, Time = 5.64s\n",
            "Step = 132000, Avg Loss = 9.8171, Time = 5.58s\n",
            "Step = 133000, Avg Loss = 9.7826, Time = 5.59s\n",
            "Step = 134000, Avg Loss = 9.7708, Time = 5.59s\n",
            "Step = 135000, Avg Loss = 9.8361, Time = 5.59s\n",
            "Step = 136000, Avg Loss = 9.8114, Time = 5.58s\n",
            "Step = 137000, Avg Loss = 9.8241, Time = 5.59s\n",
            "Step = 138000, Avg Loss = 9.8014, Time = 5.58s\n",
            "Step = 139000, Avg Loss = 9.7012, Time = 5.59s\n",
            "Step = 140000, Avg Loss = 9.7476, Time = 5.59s\n",
            "Step = 141000, Avg Loss = 9.8059, Time = 5.59s\n",
            "Step = 142000, Avg Loss = 9.8590, Time = 5.60s\n",
            "Step = 143000, Avg Loss = 9.8950, Time = 5.60s\n",
            "Step = 144000, Avg Loss = 9.8384, Time = 5.60s\n",
            "Step = 145000, Avg Loss = 9.8990, Time = 5.59s\n",
            "Step = 146000, Avg Loss = 9.8517, Time = 5.59s\n",
            "Step = 147000, Avg Loss = 9.8030, Time = 5.59s\n",
            "Step = 148000, Avg Loss = 9.8614, Time = 5.78s\n",
            "Step = 149000, Avg Loss = 9.9416, Time = 5.59s\n",
            "Step = 150000, Avg Loss = 9.9803, Time = 5.59s\n",
            "Step = 151000, Avg Loss = 9.9994, Time = 5.60s\n",
            "Step = 152000, Avg Loss = 10.0536, Time = 5.59s\n",
            "Step = 153000, Avg Loss = 10.1022, Time = 5.60s\n",
            "Step = 154000, Avg Loss = 10.1729, Time = 5.59s\n",
            "Step = 155000, Avg Loss = 10.2402, Time = 5.59s\n",
            "Step = 156000, Avg Loss = 10.2116, Time = 5.60s\n",
            "Step = 157000, Avg Loss = 10.2560, Time = 5.59s\n",
            "Step = 158000, Avg Loss = 10.3313, Time = 5.59s\n",
            "Step = 159000, Avg Loss = 10.3303, Time = 5.60s\n",
            "Step = 160000, Avg Loss = 10.3700, Time = 5.59s\n",
            "Step = 161000, Avg Loss = 10.4388, Time = 5.59s\n",
            "Step = 162000, Avg Loss = 10.4057, Time = 5.59s\n",
            "Step = 163000, Avg Loss = 10.5130, Time = 5.59s\n",
            "Step = 164000, Avg Loss = 10.4947, Time = 5.60s\n",
            "Step = 165000, Avg Loss = 10.5285, Time = 5.60s\n",
            "Step = 166000, Avg Loss = 10.5381, Time = 5.60s\n",
            "Step = 167000, Avg Loss = 10.6168, Time = 5.59s\n",
            "Step = 168000, Avg Loss = 10.6853, Time = 5.59s\n",
            "Step = 169000, Avg Loss = 10.7042, Time = 5.59s\n",
            "Step = 170000, Avg Loss = 10.7413, Time = 5.60s\n",
            "Step = 171000, Avg Loss = 10.7173, Time = 5.59s\n",
            "Step = 172000, Avg Loss = 10.7719, Time = 5.60s\n",
            "Step = 173000, Avg Loss = 10.8130, Time = 5.58s\n",
            "Step = 174000, Avg Loss = 10.7850, Time = 5.59s\n",
            "Step = 175000, Avg Loss = 10.8358, Time = 5.60s\n",
            "Step = 176000, Avg Loss = 10.8729, Time = 5.59s\n",
            "Step = 177000, Avg Loss = 10.9461, Time = 5.60s\n",
            "Step = 178000, Avg Loss = 10.9827, Time = 5.59s\n",
            "Step = 179000, Avg Loss = 11.0378, Time = 5.59s\n",
            "Step = 180000, Avg Loss = 11.0788, Time = 5.59s\n",
            "Step = 181000, Avg Loss = 11.1793, Time = 5.59s\n",
            "Step = 182000, Avg Loss = 11.2081, Time = 5.59s\n",
            "Step = 183000, Avg Loss = 11.2068, Time = 5.59s\n",
            "Step = 184000, Avg Loss = 11.3986, Time = 5.59s\n",
            "Step = 185000, Avg Loss = 11.3552, Time = 5.59s\n",
            "Step = 186000, Avg Loss = 11.4444, Time = 5.60s\n",
            "Step = 187000, Avg Loss = 11.4763, Time = 5.59s\n",
            "Step = 188000, Avg Loss = 11.5862, Time = 5.59s\n",
            "Step = 189000, Avg Loss = 11.5788, Time = 5.58s\n",
            "Step = 190000, Avg Loss = 11.6834, Time = 5.59s\n",
            "Step = 191000, Avg Loss = 11.7981, Time = 5.59s\n",
            "Step = 192000, Avg Loss = 11.9166, Time = 5.58s\n",
            "Step = 193000, Avg Loss = 11.9931, Time = 5.59s\n",
            "Step = 194000, Avg Loss = 11.9575, Time = 5.59s\n",
            "Step = 195000, Avg Loss = 12.0704, Time = 5.59s\n",
            "Step = 196000, Avg Loss = 12.1218, Time = 5.59s\n",
            "Step = 197000, Avg Loss = 12.2323, Time = 5.59s\n",
            "Step = 198000, Avg Loss = 12.2925, Time = 5.60s\n",
            "Step = 199000, Avg Loss = 12.4300, Time = 5.60s\n",
            "Step = 200000, Avg Loss = 12.3919, Time = 5.59s\n",
            "Step = 201000, Avg Loss = 12.6109, Time = 5.59s\n",
            "Step = 202000, Avg Loss = 12.5516, Time = 5.60s\n",
            "Step = 203000, Avg Loss = 12.6383, Time = 5.59s\n",
            "Step = 204000, Avg Loss = 12.7694, Time = 5.60s\n",
            "Step = 205000, Avg Loss = 12.9171, Time = 5.60s\n",
            "Step = 206000, Avg Loss = 13.0919, Time = 5.60s\n",
            "Step = 207000, Avg Loss = 13.0522, Time = 5.60s\n",
            "Step = 208000, Avg Loss = 13.2145, Time = 5.59s\n",
            "Step = 209000, Avg Loss = 13.3710, Time = 5.60s\n",
            "Step = 210000, Avg Loss = 13.5150, Time = 5.60s\n",
            "Step = 211000, Avg Loss = 13.5253, Time = 5.60s\n",
            "Step = 212000, Avg Loss = 13.6106, Time = 5.59s\n",
            "Step = 213000, Avg Loss = 13.7729, Time = 5.59s\n",
            "Step = 214000, Avg Loss = 13.9085, Time = 5.60s\n",
            "Step = 215000, Avg Loss = 13.9047, Time = 5.59s\n",
            "Step = 216000, Avg Loss = 13.9048, Time = 5.59s\n",
            "Step = 217000, Avg Loss = 13.9346, Time = 5.60s\n",
            "Step = 218000, Avg Loss = 14.0708, Time = 5.60s\n",
            "Step = 219000, Avg Loss = 14.0945, Time = 5.60s\n",
            "Step = 220000, Avg Loss = 14.1655, Time = 5.60s\n",
            "Step = 221000, Avg Loss = 14.3094, Time = 5.60s\n",
            "Step = 222000, Avg Loss = 14.4527, Time = 5.60s\n",
            "Step = 223000, Avg Loss = 14.4691, Time = 5.59s\n",
            "Step = 224000, Avg Loss = 14.5813, Time = 5.59s\n",
            "Step = 225000, Avg Loss = 14.4005, Time = 5.59s\n",
            "Step = 226000, Avg Loss = 14.1654, Time = 5.60s\n",
            "Step = 227000, Avg Loss = 14.2472, Time = 5.60s\n",
            "Step = 228000, Avg Loss = 14.3561, Time = 5.60s\n",
            "Step = 229000, Avg Loss = 14.5203, Time = 5.60s\n",
            "Step = 230000, Avg Loss = 14.6676, Time = 5.60s\n",
            "Step = 231000, Avg Loss = 14.5548, Time = 5.60s\n",
            "Step = 232000, Avg Loss = 14.3641, Time = 5.60s\n",
            "Step = 233000, Avg Loss = 14.6269, Time = 5.60s\n",
            "Step = 234000, Avg Loss = 14.8185, Time = 5.59s\n",
            "Step = 235000, Avg Loss = 14.8058, Time = 5.62s\n",
            "Step = 236000, Avg Loss = 14.8016, Time = 5.64s\n",
            "Step = 237000, Avg Loss = 14.4061, Time = 5.61s\n",
            "Step = 238000, Avg Loss = 14.2012, Time = 5.59s\n",
            "Step = 239000, Avg Loss = 14.1032, Time = 5.59s\n",
            "Step = 240000, Avg Loss = 14.0695, Time = 5.60s\n",
            "Step = 241000, Avg Loss = 14.0618, Time = 5.60s\n",
            "Step = 242000, Avg Loss = 14.2418, Time = 5.59s\n",
            "Step = 243000, Avg Loss = 14.4715, Time = 5.60s\n",
            "Step = 244000, Avg Loss = 14.5428, Time = 5.59s\n",
            "Step = 245000, Avg Loss = 14.5153, Time = 5.58s\n",
            "Step = 246000, Avg Loss = 14.5196, Time = 5.59s\n",
            "Step = 247000, Avg Loss = 14.6413, Time = 5.59s\n",
            "Step = 248000, Avg Loss = 14.7225, Time = 5.59s\n",
            "Step = 249000, Avg Loss = 14.7473, Time = 5.59s\n",
            "Step = 250000, Avg Loss = 14.4641, Time = 5.60s\n",
            "Step = 251000, Avg Loss = 14.3010, Time = 5.59s\n",
            "Step = 252000, Avg Loss = 14.3490, Time = 5.60s\n",
            "Step = 253000, Avg Loss = 14.2748, Time = 5.59s\n",
            "Step = 254000, Avg Loss = 14.3963, Time = 5.58s\n",
            "Step = 255000, Avg Loss = 14.5024, Time = 5.59s\n",
            "Step = 256000, Avg Loss = 14.4808, Time = 5.60s\n",
            "Step = 257000, Avg Loss = 14.4349, Time = 5.59s\n",
            "Step = 258000, Avg Loss = 14.3228, Time = 5.58s\n",
            "Step = 259000, Avg Loss = 14.4815, Time = 5.59s\n",
            "Step = 260000, Avg Loss = 14.3328, Time = 5.58s\n",
            "Step = 261000, Avg Loss = 14.4099, Time = 5.58s\n",
            "Step = 262000, Avg Loss = 14.4551, Time = 5.58s\n",
            "Step = 263000, Avg Loss = 14.5735, Time = 5.58s\n",
            "Step = 264000, Avg Loss = 14.5349, Time = 5.58s\n",
            "Step = 265000, Avg Loss = 14.6611, Time = 5.58s\n",
            "Step = 266000, Avg Loss = 14.6827, Time = 5.59s\n",
            "Step = 267000, Avg Loss = 14.6990, Time = 5.59s\n",
            "Step = 268000, Avg Loss = 14.6826, Time = 5.60s\n",
            "Step = 269000, Avg Loss = 14.6656, Time = 5.60s\n",
            "Step = 270000, Avg Loss = 14.7521, Time = 5.60s\n",
            "Step = 271000, Avg Loss = 14.8628, Time = 5.60s\n",
            "Step = 272000, Avg Loss = 14.8542, Time = 5.60s\n",
            "Step = 273000, Avg Loss = 14.9232, Time = 5.60s\n",
            "Step = 274000, Avg Loss = 15.0535, Time = 5.60s\n",
            "Step = 275000, Avg Loss = 15.0955, Time = 5.66s\n",
            "Step = 276000, Avg Loss = 15.2476, Time = 5.59s\n",
            "Step = 277000, Avg Loss = 15.1465, Time = 5.60s\n",
            "Step = 278000, Avg Loss = 15.1349, Time = 5.59s\n",
            "Step = 279000, Avg Loss = 15.3190, Time = 5.60s\n",
            "Step = 280000, Avg Loss = 15.2526, Time = 5.59s\n",
            "Step = 281000, Avg Loss = 15.3257, Time = 5.60s\n",
            "Step = 282000, Avg Loss = 15.3145, Time = 5.58s\n",
            "Step = 283000, Avg Loss = 15.2002, Time = 5.59s\n",
            "Step = 284000, Avg Loss = 15.2064, Time = 5.59s\n",
            "Step = 285000, Avg Loss = 15.3890, Time = 5.59s\n",
            "Step = 286000, Avg Loss = 15.4226, Time = 5.65s\n",
            "Step = 287000, Avg Loss = 15.5063, Time = 5.59s\n",
            "Step = 288000, Avg Loss = 15.5529, Time = 5.60s\n",
            "Step = 289000, Avg Loss = 15.4063, Time = 5.60s\n",
            "Step = 290000, Avg Loss = 15.4471, Time = 5.60s\n",
            "Step = 291000, Avg Loss = 15.4646, Time = 5.59s\n",
            "Step = 292000, Avg Loss = 15.5090, Time = 5.60s\n",
            "Step = 293000, Avg Loss = 15.6690, Time = 5.59s\n",
            "Step = 294000, Avg Loss = 15.8416, Time = 5.59s\n",
            "Step = 295000, Avg Loss = 16.0131, Time = 5.59s\n",
            "Step = 296000, Avg Loss = 16.2680, Time = 5.70s\n",
            "Step = 297000, Avg Loss = 16.5067, Time = 5.59s\n",
            "Step = 298000, Avg Loss = 16.6758, Time = 5.60s\n",
            "Step = 299000, Avg Loss = 16.9144, Time = 5.60s\n",
            "Step = 300000, Avg Loss = 16.9915, Time = 5.60s\n",
            "Step = 301000, Avg Loss = 17.2636, Time = 5.59s\n",
            "Step = 302000, Avg Loss = 17.0055, Time = 5.60s\n",
            "Step = 303000, Avg Loss = 16.6098, Time = 5.60s\n",
            "Step = 304000, Avg Loss = 16.4511, Time = 5.60s\n",
            "Step = 305000, Avg Loss = 16.2241, Time = 5.59s\n",
            "Step = 306000, Avg Loss = 15.9572, Time = 5.59s\n",
            "Step = 307000, Avg Loss = 15.8024, Time = 5.59s\n",
            "Step = 308000, Avg Loss = 15.9373, Time = 5.60s\n",
            "Step = 309000, Avg Loss = 16.0222, Time = 5.59s\n",
            "Step = 310000, Avg Loss = 16.3280, Time = 5.60s\n",
            "Step = 311000, Avg Loss = 16.3333, Time = 5.60s\n",
            "Step = 312000, Avg Loss = 16.3749, Time = 5.60s\n",
            "Step = 313000, Avg Loss = 16.5402, Time = 5.61s\n",
            "Step = 314000, Avg Loss = 16.4459, Time = 5.60s\n",
            "Step = 315000, Avg Loss = 16.4183, Time = 5.60s\n",
            "Step = 316000, Avg Loss = 16.5409, Time = 5.60s\n",
            "Step = 317000, Avg Loss = 16.4863, Time = 5.60s\n",
            "Step = 318000, Avg Loss = 16.6425, Time = 5.60s\n",
            "Step = 319000, Avg Loss = 16.9956, Time = 5.60s\n",
            "Step = 320000, Avg Loss = 16.7026, Time = 5.60s\n",
            "Step = 321000, Avg Loss = 16.4414, Time = 5.60s\n",
            "Step = 322000, Avg Loss = 16.3380, Time = 5.61s\n",
            "Step = 323000, Avg Loss = 16.2116, Time = 5.62s\n",
            "Step = 324000, Avg Loss = 16.3939, Time = 5.61s\n",
            "Step = 325000, Avg Loss = 16.2728, Time = 5.61s\n",
            "Step = 326000, Avg Loss = 16.3550, Time = 5.61s\n",
            "Step = 327000, Avg Loss = 16.3356, Time = 5.61s\n",
            "Step = 328000, Avg Loss = 16.5924, Time = 5.61s\n",
            "Step = 329000, Avg Loss = 16.4825, Time = 5.61s\n",
            "Step = 330000, Avg Loss = 16.4894, Time = 5.61s\n",
            "Step = 331000, Avg Loss = 16.5988, Time = 5.59s\n",
            "Step = 332000, Avg Loss = 16.4532, Time = 5.60s\n",
            "Step = 333000, Avg Loss = 16.6639, Time = 5.61s\n",
            "Step = 334000, Avg Loss = 16.4787, Time = 5.61s\n",
            "Step = 335000, Avg Loss = 16.6462, Time = 5.61s\n",
            "Step = 336000, Avg Loss = 16.5299, Time = 5.61s\n",
            "Step = 337000, Avg Loss = 16.8079, Time = 5.61s\n",
            "Step = 338000, Avg Loss = 17.1663, Time = 5.61s\n",
            "Step = 339000, Avg Loss = 17.3158, Time = 5.61s\n",
            "Step = 340000, Avg Loss = 17.4835, Time = 5.61s\n",
            "Step = 341000, Avg Loss = 17.3715, Time = 5.61s\n",
            "Step = 342000, Avg Loss = 17.6169, Time = 5.61s\n",
            "Step = 343000, Avg Loss = 18.1792, Time = 5.61s\n",
            "Step = 344000, Avg Loss = 18.3081, Time = 5.62s\n",
            "Step = 345000, Avg Loss = 18.4141, Time = 5.62s\n",
            "Step = 346000, Avg Loss = 18.3659, Time = 5.61s\n",
            "Step = 347000, Avg Loss = 18.4401, Time = 5.61s\n",
            "Step = 348000, Avg Loss = 18.6422, Time = 5.60s\n",
            "Step = 349000, Avg Loss = 18.9688, Time = 5.61s\n",
            "Step = 350000, Avg Loss = 19.2406, Time = 5.61s\n",
            "Step = 351000, Avg Loss = 19.4918, Time = 5.61s\n",
            "Step = 352000, Avg Loss = 19.5172, Time = 5.61s\n",
            "Step = 353000, Avg Loss = 19.7890, Time = 5.61s\n",
            "Step = 354000, Avg Loss = 20.2945, Time = 5.61s\n",
            "Step = 355000, Avg Loss = 20.0113, Time = 5.61s\n",
            "Step = 356000, Avg Loss = 19.1761, Time = 5.61s\n",
            "Step = 357000, Avg Loss = 18.5589, Time = 5.61s\n",
            "Step = 358000, Avg Loss = 18.1077, Time = 5.61s\n",
            "Step = 359000, Avg Loss = 17.5239, Time = 5.61s\n",
            "Step = 360000, Avg Loss = 17.3101, Time = 5.60s\n",
            "Step = 361000, Avg Loss = 17.2830, Time = 5.60s\n",
            "Step = 362000, Avg Loss = 16.9687, Time = 5.60s\n",
            "Step = 363000, Avg Loss = 17.0353, Time = 5.61s\n",
            "Step = 364000, Avg Loss = 17.0806, Time = 5.61s\n",
            "Step = 365000, Avg Loss = 16.9703, Time = 5.61s\n",
            "Step = 366000, Avg Loss = 17.1275, Time = 5.60s\n",
            "Step = 367000, Avg Loss = 17.3350, Time = 5.62s\n",
            "Step = 368000, Avg Loss = 17.5258, Time = 5.60s\n",
            "Step = 369000, Avg Loss = 17.4025, Time = 5.60s\n",
            "Step = 370000, Avg Loss = 17.3933, Time = 5.62s\n",
            "Step = 371000, Avg Loss = 17.3134, Time = 5.60s\n",
            "Step = 372000, Avg Loss = 17.4001, Time = 5.60s\n",
            "Step = 373000, Avg Loss = 17.6328, Time = 5.60s\n",
            "Step = 374000, Avg Loss = 17.4615, Time = 5.60s\n",
            "Step = 375000, Avg Loss = 17.5013, Time = 5.67s\n",
            "Step = 376000, Avg Loss = 17.6024, Time = 5.61s\n",
            "Step = 377000, Avg Loss = 17.7168, Time = 5.59s\n",
            "Step = 378000, Avg Loss = 18.2514, Time = 5.60s\n",
            "Step = 379000, Avg Loss = 18.5727, Time = 5.60s\n",
            "Step = 380000, Avg Loss = 18.7578, Time = 5.59s\n",
            "Step = 381000, Avg Loss = 18.6981, Time = 5.59s\n",
            "Step = 382000, Avg Loss = 18.8959, Time = 5.59s\n",
            "Step = 383000, Avg Loss = 18.6091, Time = 5.59s\n",
            "Step = 384000, Avg Loss = 18.5731, Time = 5.59s\n",
            "Step = 385000, Avg Loss = 18.3945, Time = 5.60s\n",
            "Step = 386000, Avg Loss = 18.5064, Time = 5.61s\n",
            "Step = 387000, Avg Loss = 18.2807, Time = 5.61s\n",
            "Step = 388000, Avg Loss = 18.0332, Time = 5.62s\n",
            "Step = 389000, Avg Loss = 18.0864, Time = 5.61s\n",
            "Step = 390000, Avg Loss = 17.8957, Time = 5.61s\n",
            "Step = 391000, Avg Loss = 17.9828, Time = 5.61s\n",
            "Step = 392000, Avg Loss = 17.7155, Time = 5.61s\n",
            "Step = 393000, Avg Loss = 17.8541, Time = 5.62s\n",
            "Step = 394000, Avg Loss = 17.8745, Time = 5.61s\n",
            "Step = 395000, Avg Loss = 17.9974, Time = 5.61s\n",
            "Step = 396000, Avg Loss = 18.0311, Time = 5.61s\n",
            "Step = 397000, Avg Loss = 18.2939, Time = 5.61s\n",
            "Step = 398000, Avg Loss = 18.1781, Time = 5.61s\n",
            "Step = 399000, Avg Loss = 17.9791, Time = 5.61s\n",
            "Step = 400000, Avg Loss = 17.9693, Time = 5.61s\n",
            "Step = 401000, Avg Loss = 18.1834, Time = 5.61s\n",
            "Step = 402000, Avg Loss = 18.1492, Time = 5.61s\n",
            "Step = 403000, Avg Loss = 18.3578, Time = 5.60s\n",
            "Step = 404000, Avg Loss = 18.3417, Time = 5.61s\n",
            "Step = 405000, Avg Loss = 18.2866, Time = 5.60s\n",
            "Step = 406000, Avg Loss = 18.2431, Time = 5.61s\n",
            "Step = 407000, Avg Loss = 18.1983, Time = 5.61s\n",
            "Step = 408000, Avg Loss = 17.7304, Time = 5.60s\n",
            "Step = 409000, Avg Loss = 17.7361, Time = 5.60s\n",
            "Step = 410000, Avg Loss = 17.7647, Time = 5.61s\n",
            "Step = 411000, Avg Loss = 17.7049, Time = 5.60s\n",
            "Step = 412000, Avg Loss = 17.5738, Time = 5.60s\n",
            "Step = 413000, Avg Loss = 17.5689, Time = 5.61s\n",
            "Step = 414000, Avg Loss = 17.6073, Time = 5.61s\n",
            "Step = 415000, Avg Loss = 17.5333, Time = 5.61s\n",
            "Step = 416000, Avg Loss = 17.7412, Time = 5.60s\n",
            "Step = 417000, Avg Loss = 17.7791, Time = 5.61s\n",
            "Step = 418000, Avg Loss = 17.9324, Time = 5.60s\n",
            "Step = 419000, Avg Loss = 18.0046, Time = 5.60s\n",
            "Step = 420000, Avg Loss = 18.0064, Time = 5.61s\n",
            "Step = 421000, Avg Loss = 18.2623, Time = 5.60s\n",
            "Step = 422000, Avg Loss = 18.1742, Time = 5.61s\n",
            "Step = 423000, Avg Loss = 18.2013, Time = 5.60s\n",
            "Step = 424000, Avg Loss = 17.8994, Time = 5.61s\n",
            "Step = 425000, Avg Loss = 18.1699, Time = 5.60s\n",
            "Step = 426000, Avg Loss = 18.2524, Time = 5.61s\n",
            "Step = 427000, Avg Loss = 18.1599, Time = 5.60s\n",
            "Step = 428000, Avg Loss = 18.1872, Time = 5.61s\n",
            "Step = 429000, Avg Loss = 18.3145, Time = 5.60s\n",
            "Step = 430000, Avg Loss = 18.3657, Time = 5.60s\n",
            "Step = 431000, Avg Loss = 18.2512, Time = 5.62s\n",
            "Step = 432000, Avg Loss = 18.3368, Time = 5.60s\n",
            "Step = 433000, Avg Loss = 17.9222, Time = 5.61s\n",
            "Step = 434000, Avg Loss = 17.8061, Time = 5.60s\n",
            "Step = 435000, Avg Loss = 17.7858, Time = 5.60s\n",
            "Step = 436000, Avg Loss = 17.7855, Time = 5.60s\n",
            "Step = 437000, Avg Loss = 17.7733, Time = 5.61s\n",
            "Step = 438000, Avg Loss = 17.8126, Time = 5.60s\n",
            "Step = 439000, Avg Loss = 17.6443, Time = 5.60s\n",
            "Step = 440000, Avg Loss = 17.9349, Time = 5.60s\n",
            "Step = 441000, Avg Loss = 17.9518, Time = 5.60s\n",
            "Step = 442000, Avg Loss = 18.0386, Time = 5.61s\n",
            "Step = 443000, Avg Loss = 18.1971, Time = 5.72s\n",
            "Step = 444000, Avg Loss = 18.2224, Time = 5.61s\n",
            "Step = 445000, Avg Loss = 18.3185, Time = 5.61s\n",
            "Step = 446000, Avg Loss = 18.2546, Time = 5.61s\n",
            "Step = 447000, Avg Loss = 18.3092, Time = 5.61s\n",
            "Step = 448000, Avg Loss = 18.2013, Time = 5.61s\n",
            "Step = 449000, Avg Loss = 18.1299, Time = 5.61s\n",
            "Step = 450000, Avg Loss = 18.2505, Time = 5.61s\n",
            "Step = 451000, Avg Loss = 18.3173, Time = 5.62s\n",
            "Step = 452000, Avg Loss = 18.1910, Time = 5.61s\n",
            "Step = 453000, Avg Loss = 18.3256, Time = 5.62s\n",
            "Step = 454000, Avg Loss = 18.5665, Time = 5.60s\n",
            "Step = 455000, Avg Loss = 18.5402, Time = 5.62s\n",
            "Step = 456000, Avg Loss = 18.7832, Time = 5.61s\n",
            "Step = 457000, Avg Loss = 18.9955, Time = 5.61s\n",
            "Step = 458000, Avg Loss = 18.8221, Time = 5.60s\n",
            "Step = 459000, Avg Loss = 19.1870, Time = 5.61s\n",
            "Step = 460000, Avg Loss = 19.3331, Time = 5.61s\n",
            "Step = 461000, Avg Loss = 19.4315, Time = 5.61s\n",
            "Step = 462000, Avg Loss = 19.6768, Time = 5.61s\n",
            "Step = 463000, Avg Loss = 19.8113, Time = 5.61s\n",
            "Step = 464000, Avg Loss = 19.6275, Time = 5.60s\n",
            "Step = 465000, Avg Loss = 19.4947, Time = 5.63s\n",
            "Step = 466000, Avg Loss = 19.6029, Time = 5.61s\n",
            "Step = 467000, Avg Loss = 19.3871, Time = 5.60s\n",
            "Step = 468000, Avg Loss = 19.5135, Time = 5.60s\n",
            "Step = 469000, Avg Loss = 19.3899, Time = 5.61s\n",
            "Step = 470000, Avg Loss = 19.1826, Time = 5.60s\n",
            "Step = 471000, Avg Loss = 19.3331, Time = 5.60s\n",
            "Step = 472000, Avg Loss = 19.2100, Time = 5.61s\n",
            "Step = 473000, Avg Loss = 19.0529, Time = 5.60s\n",
            "Step = 474000, Avg Loss = 19.1660, Time = 5.61s\n",
            "Step = 475000, Avg Loss = 19.0210, Time = 5.61s\n",
            "Step = 476000, Avg Loss = 19.0253, Time = 5.61s\n",
            "Step = 477000, Avg Loss = 18.7738, Time = 5.60s\n",
            "Step = 478000, Avg Loss = 18.8885, Time = 5.60s\n",
            "Step = 479000, Avg Loss = 18.7208, Time = 5.60s\n",
            "Step = 480000, Avg Loss = 18.7186, Time = 5.60s\n",
            "Step = 481000, Avg Loss = 18.8510, Time = 5.61s\n",
            "Step = 482000, Avg Loss = 18.8287, Time = 5.61s\n",
            "Step = 483000, Avg Loss = 18.7676, Time = 5.61s\n",
            "Step = 484000, Avg Loss = 19.1355, Time = 5.60s\n",
            "Step = 485000, Avg Loss = 19.1773, Time = 5.61s\n",
            "Step = 486000, Avg Loss = 19.0307, Time = 5.60s\n",
            "Step = 487000, Avg Loss = 19.4492, Time = 5.60s\n",
            "Step = 488000, Avg Loss = 19.3272, Time = 5.61s\n",
            "Step = 489000, Avg Loss = 19.4276, Time = 5.60s\n",
            "Step = 490000, Avg Loss = 19.4134, Time = 5.61s\n",
            "Step = 491000, Avg Loss = 19.7161, Time = 5.61s\n",
            "Step = 492000, Avg Loss = 19.6418, Time = 5.60s\n",
            "Step = 493000, Avg Loss = 19.6125, Time = 5.61s\n",
            "Step = 494000, Avg Loss = 19.5782, Time = 5.61s\n",
            "Step = 495000, Avg Loss = 19.8856, Time = 5.61s\n",
            "Step = 496000, Avg Loss = 19.7327, Time = 5.60s\n",
            "Step = 497000, Avg Loss = 19.8581, Time = 5.60s\n",
            "Step = 498000, Avg Loss = 20.0470, Time = 5.63s\n",
            "Step = 499000, Avg Loss = 19.9649, Time = 5.60s\n",
            "Step = 500000, Avg Loss = 20.3301, Time = 5.62s\n",
            "Step = 501000, Avg Loss = 20.0127, Time = 5.61s\n",
            "Step = 502000, Avg Loss = 20.2116, Time = 5.60s\n",
            "Step = 503000, Avg Loss = 20.1697, Time = 5.61s\n",
            "Step = 504000, Avg Loss = 20.1888, Time = 5.61s\n",
            "Step = 505000, Avg Loss = 20.0256, Time = 5.61s\n",
            "Step = 506000, Avg Loss = 20.3000, Time = 5.61s\n",
            "Step = 507000, Avg Loss = 20.3385, Time = 5.61s\n",
            "Step = 508000, Avg Loss = 20.5399, Time = 5.62s\n",
            "Step = 509000, Avg Loss = 20.6789, Time = 5.61s\n",
            "Step = 510000, Avg Loss = 20.9425, Time = 5.61s\n",
            "Step = 511000, Avg Loss = 21.3530, Time = 5.61s\n",
            "Step = 512000, Avg Loss = 21.1823, Time = 5.61s\n",
            "Step = 513000, Avg Loss = 21.1957, Time = 5.61s\n",
            "Step = 514000, Avg Loss = 21.4220, Time = 5.61s\n",
            "Step = 515000, Avg Loss = 21.6292, Time = 5.61s\n",
            "Step = 516000, Avg Loss = 21.7037, Time = 5.61s\n",
            "Step = 517000, Avg Loss = 21.4825, Time = 5.61s\n",
            "Step = 518000, Avg Loss = 21.4520, Time = 5.61s\n",
            "Step = 519000, Avg Loss = 21.4284, Time = 5.59s\n",
            "Step = 520000, Avg Loss = 21.3780, Time = 5.60s\n",
            "Step = 521000, Avg Loss = 21.1199, Time = 5.60s\n",
            "Step = 522000, Avg Loss = 21.1864, Time = 5.63s\n",
            "Step = 523000, Avg Loss = 21.3652, Time = 5.61s\n",
            "Step = 524000, Avg Loss = 21.2538, Time = 5.60s\n",
            "Step = 525000, Avg Loss = 21.2491, Time = 5.61s\n",
            "Step = 526000, Avg Loss = 21.3418, Time = 5.61s\n",
            "Step = 527000, Avg Loss = 21.1686, Time = 5.61s\n",
            "Step = 528000, Avg Loss = 21.4890, Time = 5.61s\n",
            "Step = 529000, Avg Loss = 21.2152, Time = 5.61s\n",
            "Step = 530000, Avg Loss = 21.1285, Time = 5.61s\n",
            "Step = 531000, Avg Loss = 20.9932, Time = 5.61s\n",
            "Step = 532000, Avg Loss = 21.2803, Time = 5.60s\n",
            "Step = 533000, Avg Loss = 21.0047, Time = 5.60s\n",
            "Step = 534000, Avg Loss = 21.1743, Time = 5.61s\n",
            "Step = 535000, Avg Loss = 20.7634, Time = 5.61s\n",
            "Step = 536000, Avg Loss = 20.6432, Time = 5.60s\n",
            "Step = 537000, Avg Loss = 20.6303, Time = 5.60s\n",
            "Step = 538000, Avg Loss = 20.6266, Time = 5.60s\n",
            "Step = 539000, Avg Loss = 20.7472, Time = 5.62s\n",
            "Step = 540000, Avg Loss = 20.4472, Time = 5.60s\n",
            "Step = 541000, Avg Loss = 20.6550, Time = 5.61s\n",
            "Step = 542000, Avg Loss = 20.4443, Time = 5.60s\n",
            "Step = 543000, Avg Loss = 20.4472, Time = 5.60s\n",
            "Step = 544000, Avg Loss = 20.2133, Time = 5.61s\n",
            "Step = 545000, Avg Loss = 20.0817, Time = 5.61s\n",
            "Step = 546000, Avg Loss = 19.9840, Time = 5.61s\n",
            "Step = 547000, Avg Loss = 20.0303, Time = 5.60s\n",
            "Step = 548000, Avg Loss = 20.1756, Time = 5.61s\n",
            "Step = 549000, Avg Loss = 20.1132, Time = 5.60s\n",
            "Step = 550000, Avg Loss = 19.9965, Time = 5.61s\n",
            "Step = 551000, Avg Loss = 19.7708, Time = 5.61s\n",
            "Step = 552000, Avg Loss = 20.0240, Time = 5.61s\n",
            "Step = 553000, Avg Loss = 19.9187, Time = 5.61s\n",
            "Step = 554000, Avg Loss = 19.7312, Time = 5.61s\n",
            "Step = 555000, Avg Loss = 19.8440, Time = 5.61s\n",
            "Step = 556000, Avg Loss = 19.9540, Time = 5.61s\n",
            "Step = 557000, Avg Loss = 19.9857, Time = 5.61s\n",
            "Step = 558000, Avg Loss = 19.9082, Time = 5.61s\n",
            "Step = 559000, Avg Loss = 20.1520, Time = 5.62s\n",
            "Step = 560000, Avg Loss = 20.2896, Time = 5.60s\n",
            "Step = 561000, Avg Loss = 20.5119, Time = 5.63s\n",
            "Step = 562000, Avg Loss = 20.5427, Time = 5.61s\n",
            "Step = 563000, Avg Loss = 20.9513, Time = 5.61s\n",
            "Step = 564000, Avg Loss = 21.3669, Time = 5.61s\n",
            "Step = 565000, Avg Loss = 21.3337, Time = 5.61s\n",
            "Step = 566000, Avg Loss = 22.0718, Time = 5.62s\n",
            "Step = 567000, Avg Loss = 22.2647, Time = 5.61s\n",
            "Step = 568000, Avg Loss = 21.9717, Time = 5.62s\n",
            "Step = 569000, Avg Loss = 21.5174, Time = 5.61s\n",
            "Step = 570000, Avg Loss = 21.3968, Time = 5.61s\n",
            "Step = 571000, Avg Loss = 21.3835, Time = 5.60s\n",
            "Step = 572000, Avg Loss = 21.6874, Time = 5.62s\n",
            "Step = 573000, Avg Loss = 22.2756, Time = 5.60s\n",
            "Step = 574000, Avg Loss = 22.7040, Time = 5.61s\n",
            "Step = 575000, Avg Loss = 22.9815, Time = 5.61s\n",
            "Step = 576000, Avg Loss = 23.1269, Time = 5.61s\n",
            "Step = 577000, Avg Loss = 22.8671, Time = 5.62s\n",
            "Step = 578000, Avg Loss = 22.5074, Time = 5.60s\n",
            "Step = 579000, Avg Loss = 22.0631, Time = 5.61s\n",
            "Step = 580000, Avg Loss = 21.9107, Time = 5.60s\n",
            "Step = 581000, Avg Loss = 21.6097, Time = 5.60s\n",
            "Step = 582000, Avg Loss = 21.4260, Time = 5.60s\n",
            "Step = 583000, Avg Loss = 21.3723, Time = 5.61s\n",
            "Step = 584000, Avg Loss = 21.3642, Time = 5.72s\n",
            "Step = 585000, Avg Loss = 21.5170, Time = 5.60s\n",
            "Step = 586000, Avg Loss = 21.5600, Time = 5.60s\n",
            "Step = 587000, Avg Loss = 21.5883, Time = 5.60s\n",
            "Step = 588000, Avg Loss = 21.4491, Time = 5.59s\n",
            "Step = 589000, Avg Loss = 21.4875, Time = 5.59s\n",
            "Step = 590000, Avg Loss = 21.4489, Time = 5.59s\n",
            "Step = 591000, Avg Loss = 21.2564, Time = 5.71s\n",
            "Step = 592000, Avg Loss = 21.0830, Time = 5.60s\n",
            "Step = 593000, Avg Loss = 20.9827, Time = 5.61s\n",
            "Step = 594000, Avg Loss = 20.9936, Time = 5.61s\n",
            "Step = 595000, Avg Loss = 20.9624, Time = 5.61s\n",
            "Step = 596000, Avg Loss = 20.8162, Time = 5.61s\n",
            "Step = 597000, Avg Loss = 20.7258, Time = 5.61s\n",
            "Step = 598000, Avg Loss = 20.5854, Time = 5.61s\n",
            "Step = 599000, Avg Loss = 20.4273, Time = 5.60s\n",
            "Step = 600000, Avg Loss = 20.3311, Time = 5.61s\n",
            "Step = 601000, Avg Loss = 20.4944, Time = 5.60s\n",
            "Step = 602000, Avg Loss = 20.4296, Time = 5.61s\n",
            "Step = 603000, Avg Loss = 20.3167, Time = 5.61s\n",
            "Step = 604000, Avg Loss = 20.2681, Time = 5.62s\n",
            "Step = 605000, Avg Loss = 20.3562, Time = 5.62s\n",
            "Step = 606000, Avg Loss = 20.3636, Time = 5.60s\n",
            "Step = 607000, Avg Loss = 20.2569, Time = 5.59s\n",
            "Step = 608000, Avg Loss = 20.4783, Time = 5.61s\n",
            "Step = 609000, Avg Loss = 20.3497, Time = 5.61s\n",
            "Step = 610000, Avg Loss = 20.3913, Time = 5.60s\n",
            "Step = 611000, Avg Loss = 20.3849, Time = 5.59s\n",
            "Step = 612000, Avg Loss = 20.3921, Time = 5.59s\n",
            "Step = 613000, Avg Loss = 20.6283, Time = 5.60s\n",
            "Step = 614000, Avg Loss = 20.7164, Time = 5.60s\n",
            "Step = 615000, Avg Loss = 20.7898, Time = 5.60s\n",
            "Step = 616000, Avg Loss = 20.5479, Time = 5.61s\n",
            "Step = 617000, Avg Loss = 20.7711, Time = 5.62s\n",
            "Step = 618000, Avg Loss = 20.4816, Time = 5.61s\n",
            "Step = 619000, Avg Loss = 20.5546, Time = 5.61s\n",
            "Step = 620000, Avg Loss = 20.4540, Time = 5.61s\n",
            "Step = 621000, Avg Loss = 20.3841, Time = 5.60s\n",
            "Step = 622000, Avg Loss = 20.3234, Time = 5.61s\n",
            "Step = 623000, Avg Loss = 20.1946, Time = 5.60s\n",
            "Step = 624000, Avg Loss = 20.2966, Time = 5.60s\n",
            "Step = 625000, Avg Loss = 20.1416, Time = 5.61s\n",
            "Step = 626000, Avg Loss = 20.3699, Time = 5.61s\n",
            "Step = 627000, Avg Loss = 20.3348, Time = 5.60s\n",
            "Step = 628000, Avg Loss = 20.0599, Time = 5.61s\n",
            "Step = 629000, Avg Loss = 19.9831, Time = 5.60s\n",
            "Step = 630000, Avg Loss = 20.0868, Time = 5.60s\n",
            "Step = 631000, Avg Loss = 20.1562, Time = 5.60s\n",
            "Step = 632000, Avg Loss = 20.2661, Time = 5.60s\n",
            "Step = 633000, Avg Loss = 20.2399, Time = 5.60s\n",
            "Step = 634000, Avg Loss = 20.1176, Time = 5.61s\n",
            "Step = 635000, Avg Loss = 20.1877, Time = 5.60s\n",
            "Step = 636000, Avg Loss = 20.2156, Time = 5.61s\n",
            "Step = 637000, Avg Loss = 20.1677, Time = 5.61s\n",
            "Step = 638000, Avg Loss = 19.9510, Time = 5.61s\n",
            "Step = 639000, Avg Loss = 19.9619, Time = 5.61s\n",
            "Step = 640000, Avg Loss = 20.0742, Time = 5.61s\n",
            "Step = 641000, Avg Loss = 20.0841, Time = 5.61s\n",
            "Step = 642000, Avg Loss = 20.1478, Time = 5.61s\n",
            "Step = 643000, Avg Loss = 19.8276, Time = 5.61s\n",
            "Step = 644000, Avg Loss = 19.7822, Time = 5.60s\n",
            "Step = 645000, Avg Loss = 19.7459, Time = 5.62s\n",
            "Step = 646000, Avg Loss = 19.7351, Time = 5.60s\n",
            "Step = 647000, Avg Loss = 19.6857, Time = 5.61s\n",
            "Step = 648000, Avg Loss = 19.8018, Time = 5.60s\n",
            "Step = 649000, Avg Loss = 19.5696, Time = 5.61s\n",
            "Step = 650000, Avg Loss = 19.6721, Time = 5.61s\n",
            "Step = 651000, Avg Loss = 19.7277, Time = 5.61s\n",
            "Step = 652000, Avg Loss = 19.6364, Time = 5.61s\n",
            "Step = 653000, Avg Loss = 19.7335, Time = 5.60s\n",
            "Step = 654000, Avg Loss = 19.5349, Time = 5.62s\n",
            "Step = 655000, Avg Loss = 19.6563, Time = 5.62s\n",
            "Step = 656000, Avg Loss = 19.6825, Time = 5.61s\n",
            "Step = 657000, Avg Loss = 19.8861, Time = 5.61s\n",
            "Step = 658000, Avg Loss = 19.9506, Time = 5.62s\n",
            "Step = 659000, Avg Loss = 20.0256, Time = 5.60s\n",
            "Step = 660000, Avg Loss = 20.4636, Time = 5.61s\n",
            "Step = 661000, Avg Loss = 20.5909, Time = 5.61s\n",
            "Step = 662000, Avg Loss = 20.7920, Time = 5.61s\n",
            "Step = 663000, Avg Loss = 20.6426, Time = 5.62s\n",
            "Step = 664000, Avg Loss = 20.6394, Time = 5.60s\n",
            "Step = 665000, Avg Loss = 20.4163, Time = 5.62s\n",
            "Step = 666000, Avg Loss = 20.2935, Time = 5.60s\n",
            "Step = 667000, Avg Loss = 20.3176, Time = 5.61s\n",
            "Step = 668000, Avg Loss = 20.3259, Time = 5.61s\n",
            "Step = 669000, Avg Loss = 20.5309, Time = 5.62s\n",
            "Step = 670000, Avg Loss = 20.2967, Time = 5.61s\n",
            "Step = 671000, Avg Loss = 21.0119, Time = 5.62s\n",
            "Step = 672000, Avg Loss = 20.7802, Time = 5.62s\n",
            "Step = 673000, Avg Loss = 20.8783, Time = 5.61s\n",
            "Step = 674000, Avg Loss = 21.1552, Time = 5.62s\n",
            "Step = 675000, Avg Loss = 20.9988, Time = 5.60s\n",
            "Step = 676000, Avg Loss = 20.8566, Time = 5.61s\n",
            "Step = 677000, Avg Loss = 21.4157, Time = 5.61s\n",
            "Step = 678000, Avg Loss = 21.9451, Time = 5.61s\n",
            "Step = 679000, Avg Loss = 22.1902, Time = 5.63s\n",
            "Step = 680000, Avg Loss = 22.0323, Time = 5.61s\n",
            "Step = 681000, Avg Loss = 22.3604, Time = 5.61s\n",
            "Step = 682000, Avg Loss = 22.3827, Time = 5.61s\n",
            "Step = 683000, Avg Loss = 22.5120, Time = 5.61s\n",
            "Step = 684000, Avg Loss = 22.0310, Time = 5.62s\n",
            "Step = 685000, Avg Loss = 21.9145, Time = 5.61s\n",
            "Step = 686000, Avg Loss = 21.3284, Time = 5.61s\n",
            "Step = 687000, Avg Loss = 20.8529, Time = 5.61s\n",
            "Step = 688000, Avg Loss = 20.6198, Time = 5.61s\n",
            "Step = 689000, Avg Loss = 20.2256, Time = 5.61s\n",
            "Step = 690000, Avg Loss = 20.2048, Time = 5.61s\n",
            "Step = 691000, Avg Loss = 20.2026, Time = 5.61s\n",
            "Step = 692000, Avg Loss = 19.9964, Time = 5.61s\n",
            "Step = 693000, Avg Loss = 19.9366, Time = 5.61s\n",
            "Step = 694000, Avg Loss = 19.7041, Time = 5.61s\n",
            "Step = 695000, Avg Loss = 19.4808, Time = 5.63s\n",
            "Step = 696000, Avg Loss = 19.6629, Time = 5.62s\n",
            "Step = 697000, Avg Loss = 19.7138, Time = 5.61s\n",
            "Step = 698000, Avg Loss = 19.7606, Time = 5.61s\n",
            "Step = 699000, Avg Loss = 19.8359, Time = 5.61s\n",
            "Step = 700000, Avg Loss = 19.7433, Time = 5.62s\n",
            "Step = 701000, Avg Loss = 19.4536, Time = 5.62s\n",
            "Step = 702000, Avg Loss = 19.5722, Time = 5.61s\n",
            "Step = 703000, Avg Loss = 19.5283, Time = 5.61s\n",
            "Step = 704000, Avg Loss = 19.5317, Time = 5.61s\n",
            "Step = 705000, Avg Loss = 19.6257, Time = 5.61s\n",
            "Step = 706000, Avg Loss = 19.5157, Time = 5.62s\n",
            "Step = 707000, Avg Loss = 19.6697, Time = 5.61s\n",
            "Step = 708000, Avg Loss = 19.3789, Time = 5.61s\n",
            "Step = 709000, Avg Loss = 19.1211, Time = 5.60s\n",
            "Step = 710000, Avg Loss = 19.1795, Time = 5.62s\n",
            "Step = 711000, Avg Loss = 19.0256, Time = 5.61s\n",
            "Step = 712000, Avg Loss = 19.0278, Time = 5.61s\n",
            "Step = 713000, Avg Loss = 19.0386, Time = 5.61s\n",
            "Step = 714000, Avg Loss = 19.2982, Time = 5.61s\n",
            "Step = 715000, Avg Loss = 19.3047, Time = 5.61s\n",
            "Step = 716000, Avg Loss = 19.5701, Time = 5.61s\n",
            "Step = 717000, Avg Loss = 19.3478, Time = 5.62s\n",
            "Step = 718000, Avg Loss = 19.5200, Time = 5.61s\n",
            "Step = 719000, Avg Loss = 19.4866, Time = 5.62s\n",
            "Step = 720000, Avg Loss = 19.4605, Time = 5.61s\n",
            "Step = 721000, Avg Loss = 19.5281, Time = 5.61s\n",
            "Step = 722000, Avg Loss = 19.7299, Time = 5.63s\n",
            "Step = 723000, Avg Loss = 19.6299, Time = 5.62s\n",
            "Step = 724000, Avg Loss = 19.9271, Time = 5.61s\n",
            "Step = 725000, Avg Loss = 20.0090, Time = 5.62s\n",
            "Step = 726000, Avg Loss = 20.3643, Time = 5.61s\n",
            "Step = 727000, Avg Loss = 20.4013, Time = 5.62s\n",
            "Step = 728000, Avg Loss = 20.3618, Time = 5.61s\n",
            "Step = 729000, Avg Loss = 20.5957, Time = 5.61s\n",
            "Step = 730000, Avg Loss = 20.7118, Time = 5.61s\n",
            "Step = 731000, Avg Loss = 20.9818, Time = 5.61s\n",
            "Step = 732000, Avg Loss = 21.2156, Time = 5.61s\n",
            "Step = 733000, Avg Loss = 21.2425, Time = 5.61s\n",
            "Step = 734000, Avg Loss = 21.5655, Time = 5.61s\n",
            "Step = 735000, Avg Loss = 21.6704, Time = 5.62s\n",
            "Step = 736000, Avg Loss = 22.0336, Time = 5.61s\n",
            "Step = 737000, Avg Loss = 21.8430, Time = 5.61s\n",
            "Step = 738000, Avg Loss = 22.0355, Time = 5.62s\n",
            "Step = 739000, Avg Loss = 22.4760, Time = 5.72s\n",
            "Step = 740000, Avg Loss = 22.8870, Time = 5.61s\n",
            "Step = 741000, Avg Loss = 22.5317, Time = 5.61s\n",
            "Step = 742000, Avg Loss = 22.0242, Time = 5.61s\n",
            "Step = 743000, Avg Loss = 21.8425, Time = 5.61s\n",
            "Step = 744000, Avg Loss = 21.4779, Time = 5.62s\n",
            "Step = 745000, Avg Loss = 21.3755, Time = 5.61s\n",
            "Step = 746000, Avg Loss = 20.8446, Time = 5.61s\n",
            "Step = 747000, Avg Loss = 20.6470, Time = 5.61s\n",
            "Step = 748000, Avg Loss = 21.0550, Time = 5.61s\n",
            "Step = 749000, Avg Loss = 21.1243, Time = 5.62s\n",
            "Step = 750000, Avg Loss = 21.0584, Time = 5.60s\n",
            "Step = 751000, Avg Loss = 21.0138, Time = 5.60s\n",
            "Step = 752000, Avg Loss = 21.2425, Time = 5.61s\n",
            "Step = 753000, Avg Loss = 20.9909, Time = 5.60s\n",
            "Step = 754000, Avg Loss = 20.8870, Time = 5.62s\n",
            "Step = 755000, Avg Loss = 20.6504, Time = 5.63s\n",
            "Step = 756000, Avg Loss = 20.9374, Time = 5.60s\n",
            "Step = 757000, Avg Loss = 20.8506, Time = 5.62s\n",
            "Step = 758000, Avg Loss = 20.9798, Time = 5.60s\n",
            "Step = 759000, Avg Loss = 21.1894, Time = 5.61s\n",
            "Step = 760000, Avg Loss = 21.4919, Time = 5.60s\n",
            "Step = 761000, Avg Loss = 21.7523, Time = 5.58s\n",
            "Step = 762000, Avg Loss = 21.9654, Time = 5.59s\n",
            "Step = 763000, Avg Loss = 22.4100, Time = 5.58s\n",
            "Step = 764000, Avg Loss = 22.9061, Time = 5.60s\n",
            "Step = 765000, Avg Loss = 23.1237, Time = 5.60s\n",
            "Step = 766000, Avg Loss = 23.6498, Time = 5.60s\n",
            "Step = 767000, Avg Loss = 24.2914, Time = 5.59s\n",
            "Step = 768000, Avg Loss = 24.6205, Time = 5.60s\n",
            "Step = 769000, Avg Loss = 24.0284, Time = 5.59s\n",
            "Step = 770000, Avg Loss = 23.8581, Time = 5.58s\n",
            "Step = 771000, Avg Loss = 23.7752, Time = 5.59s\n",
            "Step = 772000, Avg Loss = 23.9960, Time = 5.59s\n",
            "Step = 773000, Avg Loss = 24.1839, Time = 5.59s\n",
            "Step = 774000, Avg Loss = 24.4072, Time = 5.60s\n",
            "Step = 775000, Avg Loss = 24.0926, Time = 5.61s\n",
            "Step = 776000, Avg Loss = 24.1016, Time = 5.61s\n",
            "Step = 777000, Avg Loss = 24.0171, Time = 5.60s\n",
            "Step = 778000, Avg Loss = 23.9485, Time = 5.61s\n",
            "Step = 779000, Avg Loss = 23.6479, Time = 5.61s\n",
            "Step = 780000, Avg Loss = 23.9845, Time = 5.61s\n",
            "Step = 781000, Avg Loss = 24.8006, Time = 5.60s\n",
            "Step = 782000, Avg Loss = 26.2873, Time = 5.61s\n",
            "Step = 783000, Avg Loss = 26.8571, Time = 5.59s\n",
            "Step = 784000, Avg Loss = 26.6029, Time = 5.59s\n",
            "Step = 785000, Avg Loss = 26.3564, Time = 5.60s\n",
            "Step = 786000, Avg Loss = 25.4620, Time = 5.60s\n",
            "Step = 787000, Avg Loss = 25.0859, Time = 5.59s\n",
            "Step = 788000, Avg Loss = 24.4105, Time = 5.58s\n",
            "Step = 789000, Avg Loss = 23.0257, Time = 5.58s\n",
            "Step = 790000, Avg Loss = 22.2219, Time = 5.59s\n",
            "Step = 791000, Avg Loss = 21.6912, Time = 5.60s\n",
            "Step = 792000, Avg Loss = 21.5305, Time = 5.60s\n",
            "Step = 793000, Avg Loss = 21.7387, Time = 5.61s\n",
            "Step = 794000, Avg Loss = 22.0449, Time = 5.59s\n",
            "Step = 795000, Avg Loss = 22.3364, Time = 5.61s\n",
            "Step = 796000, Avg Loss = 22.8079, Time = 5.60s\n",
            "Step = 797000, Avg Loss = 22.7536, Time = 5.60s\n",
            "Step = 798000, Avg Loss = 23.4024, Time = 5.62s\n",
            "Step = 799000, Avg Loss = 25.0785, Time = 5.61s\n",
            "Step = 800000, Avg Loss = 26.4927, Time = 5.60s\n",
            "Step = 801000, Avg Loss = 27.5551, Time = 5.61s\n",
            "Step = 802000, Avg Loss = 29.0614, Time = 5.60s\n",
            "Step = 803000, Avg Loss = 30.7387, Time = 5.62s\n",
            "Step = 804000, Avg Loss = 31.9493, Time = 5.60s\n",
            "Step = 805000, Avg Loss = 35.0053, Time = 5.61s\n",
            "Step = 806000, Avg Loss = 37.1658, Time = 5.61s\n",
            "Step = 807000, Avg Loss = 37.9954, Time = 5.61s\n",
            "Step = 808000, Avg Loss = 39.0556, Time = 5.62s\n",
            "Step = 809000, Avg Loss = 40.0056, Time = 5.61s\n",
            "Step = 810000, Avg Loss = 41.8255, Time = 5.60s\n",
            "Step = 811000, Avg Loss = 42.7908, Time = 5.61s\n",
            "Step = 812000, Avg Loss = 41.8386, Time = 5.60s\n",
            "Step = 813000, Avg Loss = 38.7914, Time = 5.58s\n",
            "Step = 814000, Avg Loss = 37.5135, Time = 5.60s\n",
            "Step = 815000, Avg Loss = 35.6105, Time = 5.60s\n",
            "Step = 816000, Avg Loss = 31.9592, Time = 5.61s\n",
            "Step = 817000, Avg Loss = 28.8420, Time = 5.61s\n",
            "Step = 818000, Avg Loss = 25.9393, Time = 5.60s\n",
            "Step = 819000, Avg Loss = 24.8331, Time = 5.61s\n",
            "Step = 820000, Avg Loss = 23.6855, Time = 5.61s\n",
            "Step = 821000, Avg Loss = 23.0619, Time = 5.61s\n",
            "Step = 822000, Avg Loss = 22.9516, Time = 5.61s\n",
            "Step = 823000, Avg Loss = 22.6741, Time = 5.60s\n",
            "Step = 824000, Avg Loss = 23.0285, Time = 5.60s\n",
            "Step = 825000, Avg Loss = 22.9583, Time = 5.60s\n",
            "Step = 826000, Avg Loss = 23.2699, Time = 5.60s\n",
            "Step = 827000, Avg Loss = 24.0829, Time = 5.61s\n",
            "Step = 828000, Avg Loss = 24.1027, Time = 5.61s\n",
            "Step = 829000, Avg Loss = 24.0010, Time = 5.61s\n",
            "Step = 830000, Avg Loss = 24.8551, Time = 5.62s\n",
            "Step = 831000, Avg Loss = 25.3562, Time = 5.60s\n",
            "Step = 832000, Avg Loss = 26.5362, Time = 5.60s\n",
            "Step = 833000, Avg Loss = 26.6909, Time = 5.60s\n",
            "Step = 834000, Avg Loss = 25.8623, Time = 5.60s\n",
            "Step = 835000, Avg Loss = 26.3153, Time = 5.60s\n",
            "Step = 836000, Avg Loss = 26.4415, Time = 5.61s\n",
            "Step = 837000, Avg Loss = 26.9593, Time = 5.61s\n",
            "Step = 838000, Avg Loss = 27.3766, Time = 5.61s\n",
            "Step = 839000, Avg Loss = 28.4934, Time = 5.58s\n",
            "Step = 840000, Avg Loss = 27.6544, Time = 5.60s\n",
            "Step = 841000, Avg Loss = 28.7243, Time = 5.61s\n",
            "Step = 842000, Avg Loss = 28.6567, Time = 5.65s\n",
            "Step = 843000, Avg Loss = 28.2151, Time = 5.58s\n",
            "Step = 844000, Avg Loss = 28.7379, Time = 5.60s\n",
            "Step = 845000, Avg Loss = 29.4603, Time = 5.59s\n",
            "Step = 846000, Avg Loss = 29.4617, Time = 5.59s\n",
            "Step = 847000, Avg Loss = 29.9899, Time = 5.58s\n",
            "Step = 848000, Avg Loss = 31.4630, Time = 5.59s\n",
            "Step = 849000, Avg Loss = 31.5971, Time = 5.59s\n",
            "Step = 850000, Avg Loss = 32.2129, Time = 5.59s\n",
            "Step = 851000, Avg Loss = 33.3526, Time = 5.60s\n",
            "Step = 852000, Avg Loss = 33.8930, Time = 5.60s\n",
            "Step = 853000, Avg Loss = 35.5641, Time = 5.61s\n",
            "Step = 854000, Avg Loss = 36.9137, Time = 5.60s\n",
            "Step = 855000, Avg Loss = 37.7813, Time = 5.60s\n",
            "Step = 856000, Avg Loss = 39.1295, Time = 5.60s\n",
            "Step = 857000, Avg Loss = 42.0591, Time = 5.58s\n",
            "Step = 858000, Avg Loss = 42.9596, Time = 5.58s\n",
            "Step = 859000, Avg Loss = 43.9627, Time = 5.59s\n",
            "Step = 860000, Avg Loss = 44.7260, Time = 5.59s\n",
            "Step = 861000, Avg Loss = 45.5938, Time = 5.59s\n",
            "Step = 862000, Avg Loss = 45.8804, Time = 5.59s\n",
            "Step = 863000, Avg Loss = 46.4533, Time = 5.59s\n",
            "Step = 864000, Avg Loss = 45.8838, Time = 5.59s\n",
            "Step = 865000, Avg Loss = 47.3916, Time = 5.60s\n",
            "Step = 866000, Avg Loss = 49.8389, Time = 5.59s\n",
            "Step = 867000, Avg Loss = 51.4882, Time = 5.59s\n",
            "Step = 868000, Avg Loss = 53.7323, Time = 5.59s\n",
            "Step = 869000, Avg Loss = 54.8268, Time = 5.59s\n",
            "Step = 870000, Avg Loss = 56.6188, Time = 5.58s\n",
            "Step = 871000, Avg Loss = 57.7610, Time = 5.59s\n",
            "Step = 872000, Avg Loss = 57.1095, Time = 5.60s\n",
            "Step = 873000, Avg Loss = 58.0820, Time = 5.59s\n",
            "Step = 874000, Avg Loss = 56.4949, Time = 5.59s\n",
            "Step = 875000, Avg Loss = 52.0351, Time = 5.59s\n",
            "Step = 876000, Avg Loss = 48.6897, Time = 5.60s\n",
            "Step = 877000, Avg Loss = 41.6141, Time = 5.58s\n",
            "Step = 878000, Avg Loss = 37.5185, Time = 5.59s\n",
            "Step = 879000, Avg Loss = 35.5857, Time = 5.59s\n",
            "Step = 880000, Avg Loss = 33.9731, Time = 5.58s\n",
            "Step = 881000, Avg Loss = 32.4837, Time = 5.59s\n",
            "Step = 882000, Avg Loss = 32.6975, Time = 5.58s\n",
            "Step = 883000, Avg Loss = 31.9694, Time = 5.59s\n",
            "Step = 884000, Avg Loss = 31.5537, Time = 5.59s\n",
            "Step = 885000, Avg Loss = 31.4612, Time = 5.58s\n",
            "Step = 886000, Avg Loss = 31.0238, Time = 5.70s\n",
            "Step = 887000, Avg Loss = 31.8292, Time = 5.58s\n",
            "Step = 888000, Avg Loss = 32.2474, Time = 5.58s\n",
            "Step = 889000, Avg Loss = 32.6990, Time = 5.59s\n",
            "Step = 890000, Avg Loss = 33.1036, Time = 5.59s\n",
            "Step = 891000, Avg Loss = 33.3189, Time = 5.59s\n",
            "Step = 892000, Avg Loss = 34.2256, Time = 5.59s\n",
            "Step = 893000, Avg Loss = 34.2067, Time = 5.59s\n",
            "Step = 894000, Avg Loss = 32.8248, Time = 5.60s\n",
            "Step = 895000, Avg Loss = 33.0907, Time = 5.60s\n",
            "Step = 896000, Avg Loss = 33.7399, Time = 5.59s\n",
            "Step = 897000, Avg Loss = 32.4008, Time = 5.60s\n",
            "Step = 898000, Avg Loss = 31.5672, Time = 5.60s\n",
            "Step = 899000, Avg Loss = 29.4198, Time = 5.59s\n",
            "Step = 900000, Avg Loss = 28.0871, Time = 5.60s\n",
            "Step = 901000, Avg Loss = 26.8076, Time = 5.59s\n",
            "Step = 902000, Avg Loss = 26.0847, Time = 5.60s\n",
            "Step = 903000, Avg Loss = 25.7231, Time = 5.59s\n",
            "Step = 904000, Avg Loss = 25.8739, Time = 5.59s\n",
            "Step = 905000, Avg Loss = 25.8664, Time = 5.59s\n",
            "Step = 906000, Avg Loss = 26.4550, Time = 5.59s\n",
            "Step = 907000, Avg Loss = 26.3724, Time = 5.59s\n",
            "Step = 908000, Avg Loss = 27.1561, Time = 5.59s\n",
            "Step = 909000, Avg Loss = 27.7701, Time = 5.59s\n",
            "Step = 910000, Avg Loss = 27.7596, Time = 5.58s\n",
            "Step = 911000, Avg Loss = 27.3901, Time = 5.60s\n",
            "Step = 912000, Avg Loss = 28.0411, Time = 5.60s\n",
            "Step = 913000, Avg Loss = 27.7904, Time = 5.60s\n",
            "Step = 914000, Avg Loss = 27.8463, Time = 5.58s\n",
            "Step = 915000, Avg Loss = 27.1380, Time = 5.60s\n",
            "Step = 916000, Avg Loss = 26.6934, Time = 5.59s\n",
            "Step = 917000, Avg Loss = 26.4127, Time = 5.59s\n",
            "Step = 918000, Avg Loss = 26.5049, Time = 5.58s\n",
            "Step = 919000, Avg Loss = 26.0129, Time = 5.59s\n",
            "Step = 920000, Avg Loss = 25.7829, Time = 5.59s\n",
            "Step = 921000, Avg Loss = 25.5116, Time = 5.58s\n",
            "Step = 922000, Avg Loss = 25.0776, Time = 5.59s\n",
            "Step = 923000, Avg Loss = 25.0002, Time = 5.58s\n",
            "Step = 924000, Avg Loss = 25.6988, Time = 5.58s\n",
            "Step = 925000, Avg Loss = 25.7661, Time = 5.58s\n",
            "Step = 926000, Avg Loss = 26.0984, Time = 5.59s\n",
            "Step = 927000, Avg Loss = 26.4399, Time = 5.60s\n",
            "Step = 928000, Avg Loss = 27.5714, Time = 5.59s\n",
            "Step = 929000, Avg Loss = 27.7030, Time = 5.60s\n",
            "Step = 930000, Avg Loss = 28.2253, Time = 5.60s\n",
            "Step = 931000, Avg Loss = 28.9726, Time = 5.59s\n",
            "Step = 932000, Avg Loss = 29.6082, Time = 5.60s\n",
            "Step = 933000, Avg Loss = 30.1094, Time = 5.59s\n",
            "Step = 934000, Avg Loss = 30.2054, Time = 5.59s\n",
            "Step = 935000, Avg Loss = 30.4813, Time = 5.60s\n",
            "Step = 936000, Avg Loss = 31.1082, Time = 5.58s\n",
            "Step = 937000, Avg Loss = 31.7718, Time = 5.59s\n",
            "Step = 938000, Avg Loss = 33.5122, Time = 5.59s\n",
            "Step = 939000, Avg Loss = 34.5852, Time = 5.59s\n",
            "Step = 940000, Avg Loss = 37.1043, Time = 5.59s\n",
            "Step = 941000, Avg Loss = 42.4016, Time = 5.59s\n",
            "Step = 942000, Avg Loss = 46.4820, Time = 5.59s\n",
            "Step = 943000, Avg Loss = 51.4855, Time = 5.59s\n",
            "Step = 944000, Avg Loss = 55.4150, Time = 5.58s\n",
            "Step = 945000, Avg Loss = 60.1579, Time = 5.59s\n",
            "Step = 946000, Avg Loss = 63.0371, Time = 5.70s\n",
            "Step = 947000, Avg Loss = 67.7766, Time = 5.58s\n",
            "Step = 948000, Avg Loss = 68.6523, Time = 5.58s\n",
            "Step = 949000, Avg Loss = 68.6423, Time = 5.60s\n",
            "Step = 950000, Avg Loss = 67.1794, Time = 5.59s\n",
            "Step = 951000, Avg Loss = 65.4327, Time = 5.60s\n",
            "Step = 952000, Avg Loss = 61.1846, Time = 5.58s\n",
            "Step = 953000, Avg Loss = 58.5653, Time = 5.58s\n",
            "Step = 954000, Avg Loss = 54.2813, Time = 5.59s\n",
            "Step = 955000, Avg Loss = 48.1132, Time = 5.59s\n",
            "Step = 956000, Avg Loss = 44.5715, Time = 5.59s\n",
            "Step = 957000, Avg Loss = 40.1024, Time = 5.59s\n",
            "Step = 958000, Avg Loss = 36.1706, Time = 5.59s\n",
            "Step = 959000, Avg Loss = 33.0128, Time = 5.58s\n",
            "Step = 960000, Avg Loss = 32.2860, Time = 5.59s\n",
            "Step = 961000, Avg Loss = 31.9469, Time = 5.59s\n",
            "Step = 962000, Avg Loss = 31.8831, Time = 5.59s\n",
            "Step = 963000, Avg Loss = 32.9965, Time = 5.59s\n",
            "Step = 964000, Avg Loss = 34.6685, Time = 5.59s\n",
            "Step = 965000, Avg Loss = 35.1173, Time = 5.59s\n",
            "Step = 966000, Avg Loss = 36.9770, Time = 5.59s\n",
            "Step = 967000, Avg Loss = 37.8864, Time = 5.59s\n",
            "Step = 968000, Avg Loss = 38.5731, Time = 5.58s\n",
            "Step = 969000, Avg Loss = 39.9220, Time = 5.59s\n",
            "Step = 970000, Avg Loss = 40.0219, Time = 5.59s\n",
            "Step = 971000, Avg Loss = 41.8619, Time = 5.58s\n",
            "Step = 972000, Avg Loss = 43.8200, Time = 5.58s\n",
            "Step = 973000, Avg Loss = 47.1937, Time = 5.58s\n",
            "Step = 974000, Avg Loss = 51.2062, Time = 5.58s\n",
            "Step = 975000, Avg Loss = 56.0176, Time = 5.58s\n",
            "Step = 976000, Avg Loss = 59.0771, Time = 5.60s\n",
            "Step = 977000, Avg Loss = 63.4560, Time = 5.58s\n",
            "Step = 978000, Avg Loss = 67.0887, Time = 5.58s\n",
            "Step = 979000, Avg Loss = 69.7278, Time = 5.58s\n",
            "Step = 980000, Avg Loss = 72.4308, Time = 5.58s\n",
            "Step = 981000, Avg Loss = 74.1936, Time = 5.60s\n",
            "Step = 982000, Avg Loss = 74.0825, Time = 5.58s\n",
            "Step = 983000, Avg Loss = 74.3421, Time = 5.59s\n",
            "Step = 984000, Avg Loss = 75.1441, Time = 5.59s\n",
            "Step = 985000, Avg Loss = 76.3520, Time = 5.58s\n",
            "Step = 986000, Avg Loss = 77.7299, Time = 5.59s\n",
            "Step = 987000, Avg Loss = 75.8373, Time = 5.58s\n",
            "Step = 988000, Avg Loss = 73.5707, Time = 5.58s\n",
            "Step = 989000, Avg Loss = 70.1616, Time = 5.58s\n",
            "Step = 990000, Avg Loss = 67.2083, Time = 5.60s\n",
            "Step = 991000, Avg Loss = 65.2509, Time = 5.58s\n",
            "Step = 992000, Avg Loss = 62.1725, Time = 5.58s\n",
            "Step = 993000, Avg Loss = 59.6433, Time = 5.58s\n",
            "Step = 994000, Avg Loss = 55.7580, Time = 5.58s\n",
            "Step = 995000, Avg Loss = 53.7013, Time = 5.58s\n",
            "Step = 996000, Avg Loss = 50.7710, Time = 5.59s\n",
            "Step = 997000, Avg Loss = 49.0557, Time = 5.59s\n",
            "Step = 998000, Avg Loss = 47.6220, Time = 5.59s\n",
            "Step = 999000, Avg Loss = 46.7705, Time = 5.60s\n",
            "Step = 1000000, Avg Loss = 46.1711, Time = 5.60s\n",
            "Step = 1001000, Avg Loss = 46.4369, Time = 5.60s\n",
            "Step = 1002000, Avg Loss = 45.0669, Time = 5.58s\n",
            "Step = 1003000, Avg Loss = 43.4101, Time = 5.59s\n",
            "Step = 1004000, Avg Loss = 41.3160, Time = 5.60s\n",
            "Step = 1005000, Avg Loss = 39.2392, Time = 5.59s\n",
            "Step = 1006000, Avg Loss = 40.2342, Time = 5.58s\n",
            "Step = 1007000, Avg Loss = 38.7924, Time = 5.58s\n",
            "Step = 1008000, Avg Loss = 37.8298, Time = 5.59s\n",
            "Step = 1009000, Avg Loss = 38.7699, Time = 5.58s\n",
            "Step = 1010000, Avg Loss = 39.0529, Time = 5.59s\n",
            "Step = 1011000, Avg Loss = 39.8972, Time = 5.59s\n",
            "Step = 1012000, Avg Loss = 42.3061, Time = 5.58s\n",
            "Step = 1013000, Avg Loss = 42.9160, Time = 5.59s\n",
            "Step = 1014000, Avg Loss = 43.7045, Time = 5.58s\n",
            "Step = 1015000, Avg Loss = 47.7497, Time = 5.58s\n",
            "Step = 1016000, Avg Loss = 47.3670, Time = 5.58s\n",
            "Step = 1017000, Avg Loss = 48.4303, Time = 5.59s\n",
            "Step = 1018000, Avg Loss = 49.8407, Time = 5.58s\n",
            "Step = 1019000, Avg Loss = 49.8534, Time = 5.59s\n",
            "Step = 1020000, Avg Loss = 48.8114, Time = 5.58s\n",
            "Step = 1021000, Avg Loss = 48.5893, Time = 5.59s\n",
            "Step = 1022000, Avg Loss = 45.4695, Time = 5.59s\n",
            "Step = 1023000, Avg Loss = 44.0190, Time = 5.59s\n",
            "Step = 1024000, Avg Loss = 42.4330, Time = 5.59s\n",
            "Step = 1025000, Avg Loss = 41.0732, Time = 5.59s\n",
            "Step = 1026000, Avg Loss = 41.8557, Time = 5.59s\n",
            "Step = 1027000, Avg Loss = 40.2025, Time = 5.59s\n",
            "Step = 1028000, Avg Loss = 40.9234, Time = 5.59s\n",
            "Step = 1029000, Avg Loss = 39.4953, Time = 5.59s\n",
            "Step = 1030000, Avg Loss = 37.2482, Time = 5.58s\n",
            "Step = 1031000, Avg Loss = 39.7182, Time = 5.59s\n",
            "Step = 1032000, Avg Loss = 39.7171, Time = 5.59s\n",
            "Step = 1033000, Avg Loss = 39.4848, Time = 5.59s\n",
            "Step = 1034000, Avg Loss = 38.5771, Time = 5.72s\n",
            "Step = 1035000, Avg Loss = 38.7831, Time = 5.59s\n",
            "Step = 1036000, Avg Loss = 38.7829, Time = 5.59s\n",
            "Step = 1037000, Avg Loss = 38.3513, Time = 5.59s\n",
            "Step = 1038000, Avg Loss = 36.6713, Time = 5.60s\n",
            "Step = 1039000, Avg Loss = 36.7976, Time = 5.59s\n",
            "Step = 1040000, Avg Loss = 35.9226, Time = 5.60s\n",
            "Step = 1041000, Avg Loss = 36.5066, Time = 5.59s\n",
            "Step = 1042000, Avg Loss = 36.3873, Time = 5.59s\n",
            "Step = 1043000, Avg Loss = 36.6095, Time = 5.59s\n",
            "Step = 1044000, Avg Loss = 38.7583, Time = 5.61s\n",
            "Step = 1045000, Avg Loss = 38.3297, Time = 5.60s\n",
            "Step = 1046000, Avg Loss = 37.1286, Time = 5.59s\n",
            "Step = 1047000, Avg Loss = 36.4516, Time = 5.59s\n",
            "Step = 1048000, Avg Loss = 36.7885, Time = 5.58s\n",
            "Step = 1049000, Avg Loss = 35.5628, Time = 5.59s\n",
            "Step = 1050000, Avg Loss = 35.7745, Time = 5.58s\n",
            "Step = 1051000, Avg Loss = 36.6435, Time = 5.59s\n",
            "Step = 1052000, Avg Loss = 37.6373, Time = 5.59s\n",
            "Step = 1053000, Avg Loss = 37.9156, Time = 5.59s\n",
            "Step = 1054000, Avg Loss = 39.3896, Time = 5.58s\n",
            "Step = 1055000, Avg Loss = 39.0832, Time = 5.59s\n",
            "Step = 1056000, Avg Loss = 40.1162, Time = 5.59s\n",
            "Step = 1057000, Avg Loss = 40.2036, Time = 5.59s\n",
            "Step = 1058000, Avg Loss = 40.0840, Time = 5.59s\n",
            "Step = 1059000, Avg Loss = 40.4656, Time = 5.60s\n",
            "Step = 1060000, Avg Loss = 42.9968, Time = 5.60s\n",
            "Step = 1061000, Avg Loss = 42.5376, Time = 5.59s\n",
            "Step = 1062000, Avg Loss = 42.6083, Time = 5.59s\n",
            "Step = 1063000, Avg Loss = 42.9195, Time = 5.59s\n",
            "Step = 1064000, Avg Loss = 44.1550, Time = 5.64s\n",
            "Step = 1065000, Avg Loss = 44.6343, Time = 5.58s\n",
            "Step = 1066000, Avg Loss = 46.2738, Time = 5.60s\n",
            "Step = 1067000, Avg Loss = 49.4617, Time = 5.59s\n",
            "Step = 1068000, Avg Loss = 51.7032, Time = 5.58s\n",
            "Step = 1069000, Avg Loss = 54.7755, Time = 5.58s\n",
            "Step = 1070000, Avg Loss = 56.8261, Time = 5.59s\n",
            "Step = 1071000, Avg Loss = 58.3264, Time = 5.58s\n",
            "Step = 1072000, Avg Loss = 56.9084, Time = 5.59s\n",
            "Step = 1073000, Avg Loss = 56.6341, Time = 5.60s\n",
            "Step = 1074000, Avg Loss = 56.0130, Time = 5.59s\n",
            "Step = 1075000, Avg Loss = 53.6042, Time = 5.59s\n",
            "Step = 1076000, Avg Loss = 54.9311, Time = 5.60s\n",
            "Step = 1077000, Avg Loss = 54.7756, Time = 5.60s\n",
            "Step = 1078000, Avg Loss = 52.3590, Time = 5.60s\n",
            "Step = 1079000, Avg Loss = 52.4867, Time = 5.60s\n",
            "Step = 1080000, Avg Loss = 51.8595, Time = 5.59s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-356d972c66d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#<4. apply optimizer>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#<5. zero grads>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg_sq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqq9kee41L4P",
        "colab_type": "text"
      },
      "source": [
        "#### Анализ\n",
        "\n",
        "Получить эмбеддинги можно скаставав такое заклинание:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWsYkNn-Hnl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = model[0].weight.cpu().data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZtxY2D01RB6",
        "colab_type": "text"
      },
      "source": [
        "Проверим, получилось ли хоть сколько-то адекватно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhDwuhDSHEDm",
        "colab_type": "code",
        "outputId": "a8988e3d-544c-4187-99cf-9174ec2e2570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(embeddings, index2word, word2index, word):\n",
        "    word_emb = embeddings[word2index[word]]\n",
        "    \n",
        "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
        "    top10 = np.argsort(similarities)[-10:]\n",
        "    \n",
        "    return [index2word[index] for index in reversed(top10)]\n",
        "\n",
        "most_similar(embeddings, index2word, word2index, 'warm')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['warm',\n",
              " 'unlimited',\n",
              " 'named',\n",
              " 'factory',\n",
              " 'acquire',\n",
              " 'format',\n",
              " 'duty',\n",
              " 'talent',\n",
              " 'bangladesh',\n",
              " 'hike']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VS1x-mO1WKS",
        "colab_type": "text"
      },
      "source": [
        "И визуализируем!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuXv2HxsAecb",
        "colab_type": "code",
        "outputId": "827720ee-5070-452a-db2c-c62c1f3d335c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "    \n",
        "    if isinstance(color, str): \n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: \n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=100)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "    \n",
        "    \n",
        "def visualize_embeddings(embeddings, index2word, word_count):\n",
        "    word_vectors = embeddings[1: word_count + 1]\n",
        "    words = index2word[1: word_count + 1]\n",
        "    \n",
        "    word_tsne = get_tsne_projection(word_vectors)\n",
        "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)\n",
        "    \n",
        "    \n",
        "visualize_embeddings(model[0], index2word, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-87be35bcd6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mvisualize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-87be35bcd6e7>\u001b[0m in \u001b[0;36mvisualize_embeddings\u001b[0;34m(embeddings, index2word, word_count)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Embedding' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGfhLR6x8D3r",
        "colab_type": "text"
      },
      "source": [
        "### Continuous Bag of Words (CBoW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UuVr2IsaYhX",
        "colab_type": "text"
      },
      "source": [
        "Альтернативный вариант модели:\n",
        "\n",
        "![](https://image.ibb.co/jnsW49/CBOW.png)\n",
        "\n",
        "Теперь по *сумме* контекстных векторов предсказывается вектор центрального слова.\n",
        "\n",
        "**Задание** Реализуйте часть функции для генерации батчей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP5VmnnjtsXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_cbow_batchs_iter(contexts, window_size, batch_size):\n",
        "    data = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "    labels = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "        \n",
        "    batchs_count = int(math.ceil(len(data) / batch_size))\n",
        "    \n",
        "    print('Initializing batchs generator with {} batchs per epoch'.format(batchs_count))\n",
        "    \n",
        "    while True:\n",
        "        <do batchs generation>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9HkY-VO61n3",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на альтернативный вариант создания модели - им мы будем пользоваться чаще всего - отнаследоваться от `nn.Module`. Схематично её использование выглядит так:\n",
        "\n",
        "```python\n",
        "class MyNetModel(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(MyNetModel, self).__init__()\n",
        "        <initialize layers>\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        <apply layers>\n",
        "        return final_output\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mHKLbMwx4c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBoWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        <apply layers>\n",
        "        return output\n",
        "      \n",
        "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "loss_function = <create loss function>\n",
        "optimizer = <create optimizer>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiIgaofEyyJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, (batch, labels) in enumerate(make_cbow_batchs_iter(contexts, window_size=2, batch_size=128)):\n",
        "    <copy-paste learning cycle>\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTEWcyYmvips",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CON4VOyG3iET",
        "colab_type": "text"
      },
      "source": [
        "### Negative Sampling\n",
        "\n",
        "Что сейчас самое тяжелое? Вычисление softmax и применение градиентов ко всем словам в $V$.\n",
        "\n",
        "Один из способов справиться с этим - использовать *Negative Sampling*.\n",
        "\n",
        "По сути, вместо предсказания индекса слова по контексту предсказывается вероятность того, что такое слово может быть в таком контексте: $P(D=1|w,c)$.\n",
        "\n",
        "Можно использовать обычную сигмоиду для получения данной вероятности: \n",
        "$$P(D=1|w, c) = \\sigma(v_w^T u_c) = \\frac 1 {1 + \\exp(-v^T_w u_c)}.$$\n",
        "\n",
        "Процесс обучения тогда выглядит так: для каждой пары слово и его контекст генерируем набор отрицательных примеров:\n",
        "\n",
        "![Negative Sampling](https://image.ibb.co/dnOUDH/Negative_Sampling.png)\n",
        "\n",
        "Для CBoW функция потерь будет выглядеть так:\n",
        "$$-\\log \\sigma(v_c^T u_c) - \\sum_{k=1}^K \\log \\sigma(-\\tilde v_k^T u_c),$$\n",
        "где $v_c$ - вектор центрального слова, $u_c$ - вектор контекста (сумма контекстных векторов), $\\tilde v_1, \\ldots, \\tilde v_K$ - сэмплированные негативные примеры.\n",
        "\n",
        "Сравните эту формулу с обычным CBoW:\n",
        "$$-v_c^T u_c + \\log \\sum_{i=1}^{|V|} \\exp(v_i^T u_c).$$\n",
        "\n",
        "Обычно слова сэмплируются из $U^{3/4}$, где $U$ - униграмное распределение, т.е частоты появления слова делённые на суммарое число слов. \n",
        "\n",
        "Частотности мы уже считали: они получаются в `Counter(words)`. Достаточно просто преобразовать их в вероятности и домножить эти вероятности на $\\frac 3 4$. Почему $\\frac 3 4$? Некоторую интуицию можно найти в следующем примере:\n",
        "\n",
        "$$P(\\text{is}) = 0.9, \\ P(\\text{is})^{3/4} = 0.92$$\n",
        "$$P(\\text{Constitution}) = 0.09, \\ P(\\text{Constitution})^{3/4} = 0.16$$\n",
        "$$P(\\text{bombastic}) = 0.01, \\ P(\\text{bombastic})^{3/4} = 0.032$$\n",
        "\n",
        "Вероятность для высокочастотных слов особо не увеличилась (относительно), зато низкочастотные будут выпадать с заметно большей вероятностей.\n",
        "\n",
        "**Задание** Реализуйте свой Negative Sampling.\n",
        "\n",
        "Для начала зададим распределение для сэмплирования:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcX4vRBLlXy6",
        "colab_type": "code",
        "outputId": "3abece20-afa7-4028-c63c-b6e0a2c373bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "words_sum_count = sum(words_counter.values())\n",
        "word_distribution = np.array([(words_counter[word] / words_sum_count) ** (3 / 4) for word in index2word])\n",
        "word_distribution /= word_distribution.sum()\n",
        "\n",
        "indices = np.arange(len(word_distribution))\n",
        "\n",
        "np.random.choice(indices, p=word_distribution, size=(32, 5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-eaf33299b251>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    np.random.choice(indices, p=word_distribution, size=(32, 5)\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2pzsue16Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NegativeSamplingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets, num_samples):\n",
        "        '''\n",
        "        inputs: (batch_size, context_size)\n",
        "        targets: (batch_size)\n",
        "        num_samples: int\n",
        "        '''\n",
        "        \n",
        "        <calculate u_c's>\n",
        "        \n",
        "        <calculate v_c>\n",
        "        \n",
        "        <sample indices>\n",
        "        <calculate negative vectors v'_c>\n",
        "        \n",
        "        <apply F.logsigmoid to v_c * u_c and to -v'_c * u_c>\n",
        "        \n",
        "        <calc result loss>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wz2iRanqzlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = NegativeSamplingModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  \n",
        "\n",
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, (batch, labels) in enumerate(make_cbow_batchs_iter(contexts, window_size=2, batch_size=128)):\n",
        "    <copy-paste (mostly) learning cycle>\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFik_6djvg3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqDmuu7m_PB5",
        "colab_type": "text"
      },
      "source": [
        "# Дополнительные материалы\n",
        "## Почитать\n",
        "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
        "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
        "[Word2Vec Tutorial - The Skip-Gram Model, ](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "\n",
        "## Посмотреть\n",
        "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
        "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM9C1i3Y-6kv",
        "colab_type": "text"
      },
      "source": [
        "# Сдача задания\n",
        "\n",
        "[Сдача](https://goo.gl/forms/rzWjQQsGpqYNz5yt1)  \n",
        "[Опрос](https://goo.gl/forms/as640TWE058bFTpy2)"
      ]
    }
  ]
}