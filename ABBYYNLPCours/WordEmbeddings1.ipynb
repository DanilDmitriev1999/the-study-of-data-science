{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Embeddings1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWzU9hqUomdU",
        "colab_type": "code",
        "outputId": "925e3b86-2080-4850-fca3-d6a7efdefb3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
        "!unzip quora.zip\n",
        "!pip install -q --upgrade nltk gensim bokeh pandas\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  quora.zip\n",
            "  inflating: train.csv               \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 2.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1MB 47.3MB/s \n",
            "\u001b[?25h  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.25.0; python_version >= \"3.0\", but you'll have pandas 1.0.1 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXl16AOdtlDk",
        "colab_type": "text"
      },
      "source": [
        "# Словные эмбеддинги"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErZ_TOu0vAOR",
        "colab_type": "text"
      },
      "source": [
        "*NB. Ноутбук в большой степени основан на [первом задании из релевантного ШАДовского курса](https://github.com/yandexdataschool/nlp_course/tree/master/week1_embeddings).*\n",
        "\n",
        "Все видели такие картинки (я надеюсь):\n",
        "![embeddings relations](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "*From [Vector Representations of Words, Tensorflow tutorial](https://www.tensorflow.org/tutorials/representation/word2vec)*\n",
        "\n",
        "Сегодня будем заниматься такими моделями.\n",
        "\n",
        "Начнём утро с визуализаций. Идём на сайт [http://rusvectores.org/ru/](http://rusvectores.org/ru/) и смотрим, что умеют обученные модели для русского.\n",
        "\n",
        "Обратите внимание на разделы *Похожие слова* и *Калькулятор*, а также на набор моделей, которые в них можно выбирать."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkxsGQqZNjxj",
        "colab_type": "text"
      },
      "source": [
        "## Тренируем простую модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaOn69Bg1hH-",
        "colab_type": "text"
      },
      "source": [
        "Просто так смотреть на чужие модели, конечно, неприкольно - поэтому будем постепенно приближаться к решению конкретной задачи: [Quora Question Pairs at kaggle](https://www.kaggle.com/c/quora-question-pairs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-X7I7nc1gyS",
        "colab_type": "code",
        "outputId": "a164b782-7ad8-4389-b9c2-50f209d9b3c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "quora_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404285</th>\n",
              "      <td>404285</td>\n",
              "      <td>433578</td>\n",
              "      <td>379845</td>\n",
              "      <td>How many keywords are there in the Racket prog...</td>\n",
              "      <td>How many keywords are there in PERL Programmin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404286</th>\n",
              "      <td>404286</td>\n",
              "      <td>18840</td>\n",
              "      <td>155606</td>\n",
              "      <td>Do you believe there is life after death?</td>\n",
              "      <td>Is it true that there is life after death?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404287</th>\n",
              "      <td>404287</td>\n",
              "      <td>537928</td>\n",
              "      <td>537929</td>\n",
              "      <td>What is one coin?</td>\n",
              "      <td>What's this coin?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404288</th>\n",
              "      <td>404288</td>\n",
              "      <td>537930</td>\n",
              "      <td>537931</td>\n",
              "      <td>What is the approx annual cost of living while...</td>\n",
              "      <td>I am having little hairfall problem but I want...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404289</th>\n",
              "      <td>404289</td>\n",
              "      <td>537932</td>\n",
              "      <td>537933</td>\n",
              "      <td>What is like to have sex with cousin?</td>\n",
              "      <td>What is it like to have sex with your cousin?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>404290 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  ...  is_duplicate\n",
              "0            0  ...             0\n",
              "1            1  ...             0\n",
              "2            2  ...             0\n",
              "3            3  ...             0\n",
              "4            4  ...             0\n",
              "...        ...  ...           ...\n",
              "404285  404285  ...             0\n",
              "404286  404286  ...             1\n",
              "404287  404287  ...             0\n",
              "404288  404288  ...             0\n",
              "404289  404289  ...             0\n",
              "\n",
              "[404290 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p13HdkzWKtKe",
        "colab_type": "text"
      },
      "source": [
        "Поучим на этих текстах Word2vec из gensim. \n",
        "\n",
        "Для начала объединим все тексты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mchv4fS_21OX",
        "colab_type": "code",
        "outputId": "7df83fa0-9c7c-462d-8e91-c5599c59627c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
        "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
        "\n",
        "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
        "texts[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is the step by step guide to invest in share market in india?',\n",
              " 'What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n",
              " 'How can I increase the speed of my internet connection while using a VPN?',\n",
              " 'Why am I mentally very lonely? How can I solve it?',\n",
              " 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?',\n",
              " 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?',\n",
              " 'Should I buy tiago?',\n",
              " 'How can I be a good geologist?',\n",
              " 'When do you use シ instead of し?',\n",
              " 'Motorola (company): Can I hack my Charter Motorolla DCX3400?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hZMFAmvK5b7",
        "colab_type": "text"
      },
      "source": [
        "Для токенизации проще всего воспользоваться `nltk` (он быстрее `spacy`, но может быть хуже в отдельных случаях)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTxolf8nLM-n",
        "colab_type": "code",
        "outputId": "cea9344b-5a48-42b8-a6c3-0132be30cc5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokenize(texts[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What',\n",
              " 'is',\n",
              " 'the',\n",
              " 'step',\n",
              " 'by',\n",
              " 'step',\n",
              " 'guide',\n",
              " 'to',\n",
              " 'invest',\n",
              " 'in',\n",
              " 'share',\n",
              " 'market',\n",
              " 'in',\n",
              " 'india',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuJceE4JLRxK",
        "colab_type": "text"
      },
      "source": [
        "**Задание** Приведите все тексты к нижнему регистру и токенизируйте их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7XbnSdt4REg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_texts = [word_tokenize(texts[i].lower()) for i in range(len(texts))]\n",
        "\n",
        "assert all(isinstance(row, (list, tuple)) for row in tokenized_texts), \\\n",
        "    \"please convert each line into a list of tokens\"\n",
        "assert all(all(isinstance(tok, str) for tok in row) for row in tokenized_texts), \\\n",
        "    \"please convert each line into a list of tokens\"\n",
        "\n",
        "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
        "assert all(not is_latin(token) or token.islower() for tokens in tokenized_texts for token in tokens),\\\n",
        "    \"please lowercase each line\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irl7RotC5C_B",
        "colab_type": "code",
        "outputId": "57d66e31-1e9a-44e3-a635-870f131df859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print([' '.join(row) for row in tokenized_texts[:2]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['what is the step by step guide to invest in share market in india ?', 'what is the story of kohinoor ( koh-i-noor ) diamond ?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kj4dC3iLdwH",
        "colab_type": "text"
      },
      "source": [
        "Потренируем небольшую модель на полученных текстах:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCFRtqgUMq2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GNuiLio8M25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Word2Vec(tokenized_texts, \n",
        "                 size=32,      # embedding vector size\n",
        "                 min_count=5,  # consider words that occured at least 5 times\n",
        "                 window=5).wv  # define context as a 5-word window around the target word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JclToDJMNwTy",
        "colab_type": "text"
      },
      "source": [
        "## Изучаем полученную модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBVR2kY7LkCs",
        "colab_type": "text"
      },
      "source": [
        "Ура, теперь можно делать то же, что было на `rusvectores`.\n",
        "\n",
        "Получить вектор для слова:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk6Fgraj-j3c",
        "colab_type": "code",
        "outputId": "0a2ef700-e501-4e90-d87a-e86efdaaa537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "model.get_vector('anything')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.8634102 , -2.6280305 ,  0.09303133,  2.1656713 ,  3.3129342 ,\n",
              "        4.265833  ,  0.6607909 ,  0.91618246, -3.495318  , -1.0962586 ,\n",
              "       -2.243282  ,  1.6453412 , -0.980976  , -1.6114122 ,  1.3685737 ,\n",
              "        0.98368406,  0.24443018,  2.4802058 ,  2.3426967 , -0.8617387 ,\n",
              "       -3.8242962 , -2.1009064 ,  0.24165563,  1.8105466 , -0.6756625 ,\n",
              "       -2.1388807 ,  0.4172058 , -1.543624  ,  0.17005508,  0.27806228,\n",
              "       -1.4887167 , -3.052883  ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHKRy7HyLuxH",
        "colab_type": "text"
      },
      "source": [
        "Найти наиболее близкие слова:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toyNzyTB-p70",
        "colab_type": "code",
        "outputId": "bcf1d76d-3d30-4635-9e4a-5f1c0ccc6ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "model.most_similar('bread')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rice', 0.9661096334457397),\n",
              " ('fruit', 0.9455863833427429),\n",
              " ('wine', 0.9411824345588684),\n",
              " ('butter', 0.9336749315261841),\n",
              " ('cheese', 0.9329873919487),\n",
              " ('pasta', 0.9200602173805237),\n",
              " ('soup', 0.9192349910736084),\n",
              " ('potato', 0.9168845415115356),\n",
              " ('chocolate', 0.9164072275161743),\n",
              " ('beans', 0.9164015054702759)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A510z5gTL00E",
        "colab_type": "text"
      },
      "source": [
        "Или даже так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2A_DF5E-ucq",
        "colab_type": "code",
        "outputId": "e0ee0b88-c25d-4455-cd18-0b8eb5b809ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "model.most_similar(positive=['coder', 'money'], negative=['brain'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('photographer', 0.709350049495697),\n",
              " ('professional', 0.6820916533470154),\n",
              " ('soccer', 0.6707870960235596),\n",
              " ('volunteer', 0.6521384119987488),\n",
              " ('freelancer', 0.6516643166542053),\n",
              " ('escort', 0.6442617177963257),\n",
              " ('poker', 0.643587589263916),\n",
              " ('basketball', 0.6428779363632202),\n",
              " ('millionaire', 0.6426991820335388),\n",
              " ('writer', 0.6418194770812988)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-_uKG4vNIJv",
        "colab_type": "text"
      },
      "source": [
        "И так, конечно:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_mzQgi4L474",
        "colab_type": "code",
        "outputId": "711861ca-207c-4d5e-8e25-b824cbe8fbc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "model.most_similar([model.get_vector('politician') - model.get_vector('power') + model.get_vector('honesty')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('romantic', 0.7367050051689148),\n",
              " ('farewell', 0.6498050689697266),\n",
              " ('actress', 0.6245287656784058),\n",
              " ('famous', 0.6074085235595703),\n",
              " ('erotic', 0.6047395467758179),\n",
              " ('singer', 0.6017827987670898),\n",
              " ('birthday', 0.600946843624115),\n",
              " ('funny', 0.6008465886116028),\n",
              " ('brahmin', 0.5991036891937256),\n",
              " ('annoying', 0.5955349206924438)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK_ZMFZVYCa2",
        "colab_type": "code",
        "outputId": "2df9b70d-a386-4123-b06f-9171487e8dfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "model.most_similar([model.get_vector('king') - model.get_vector('man') + model.get_vector('woman')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.8648037910461426),\n",
              " ('queen', 0.8190836310386658),\n",
              " ('george', 0.8163221478462219),\n",
              " ('w.', 0.8129339218139648),\n",
              " ('prince', 0.8103979229927063),\n",
              " ('lee', 0.8065392374992371),\n",
              " ('paul', 0.8051936626434326),\n",
              " ('tony', 0.8004214763641357),\n",
              " ('jr.', 0.7957838773727417),\n",
              " ('bush', 0.7937395572662354)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5BAEyMyL2kx",
        "colab_type": "text"
      },
      "source": [
        "**Задание** Поищите аналогии самостоятельно. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FYuh4DKN1Fd",
        "colab_type": "text"
      },
      "source": [
        "## Визуализируем модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdT0eIEiN4Ja",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на проекции первых тысячи самых частотных слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1e9yS0zBr6j",
        "colab_type": "code",
        "outputId": "fda5645d-dcc5-4fe0-f9af-a2415965ccb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "words = sorted(model.vocab.keys(), \n",
        "               key=lambda word: model.vocab[word].count,\n",
        "               reverse=True)[:1000]\n",
        "\n",
        "print(words[::100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['?', 'money', 'study', 's', '6', 'physics', 'says', 'view', 'users', 'pressure']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yk5pgMXOESS",
        "colab_type": "text"
      },
      "source": [
        "**Задание** Постройте матрицу из эмбеддингов этих слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQu724f2CAh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors = model.vectors[[model.vocab[word].index for word in words]]\n",
        "\n",
        "assert isinstance(word_vectors, np.ndarray)\n",
        "assert word_vectors.shape == (len(words), model.vectors.shape[1])\n",
        "assert np.isfinite(word_vectors).all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsIvov3kVtML",
        "colab_type": "code",
        "outputId": "6c63e59e-cf35-45d8-9e1a-48bca28f729d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "word_vectors.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7cgxin-OTvK",
        "colab_type": "text"
      },
      "source": [
        "### PCA\n",
        "\n",
        "Простейший линейный метод сокращения размерностей - __P__rincipial __C__omponent __A__nalysis.\n",
        "\n",
        "PCA ищет оси, при проекции на которые данные будут иметь наибольший разброс.\n",
        "\n",
        "![pca](https://i.stack.imgur.com/Q7HIP.gif)\n",
        "*From [https://stats.stackexchange.com/a/140579](https://stats.stackexchange.com/a/140579)*\n",
        "\n",
        "В результате, можно взять проекции на несколько первых компонент - и сохранить как можно больше информации, сократив размерность.\n",
        "\n",
        "Красивые визуализации можно найти [здесь](http://setosa.io/ev/principal-component-analysis/).\n",
        "\n",
        "**Задание** Воспользуйтесь PCA из sklearn, а потом центрируйте и нормируйте результат."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0fQKZw2Css4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def get_pca_projection(word_vectors):\n",
        "    scaler = StandardScaler()\n",
        "    pca = PCA(n_components=2)\n",
        "    pca.fit(word_vectors)\n",
        "    score = pca.transform(word_vectors)\n",
        "    scaler.fit(score)\n",
        "    dt = scaler.transform(score)\n",
        "    return dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q7bzCVoabP_",
        "colab_type": "code",
        "outputId": "c31ccb74-f8c9-4da2-fc11-231c0a8c91be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "word_vectors_pca = get_pca_projection(word_vectors)\n",
        "word_vectors_pca"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.39172366, -0.27827695],\n",
              "       [-0.25650862, -0.31886962],\n",
              "       [-0.3847053 , -0.01431462],\n",
              "       ...,\n",
              "       [-0.9391109 ,  0.9504084 ],\n",
              "       [-0.35831118, -0.09274316],\n",
              "       [-1.2814859 ,  0.11919259]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_9VtLl9CviW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors_pca = get_pca_projection(word_vectors)\n",
        "\n",
        "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
        "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
        "assert max(abs(1 - word_vectors_pca.std(0))) < 1e-5, \"points must have unit variance\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGyvhrk7Rlyt",
        "colab_type": "text"
      },
      "source": [
        "Визуализируем то, что получилось:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U58YxF3Cx0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "    \n",
        "    if isinstance(color, str): \n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: \n",
        "        pl.show(fig)\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBljy2hCC1qX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOIU8uXnSItf",
        "colab_type": "text"
      },
      "source": [
        "### TSNE\n",
        "\n",
        "Более интересный и сложный (нелинейный) метод для визуализации высокоразмерных пространств - TSNE. Подробно посмотреть на него можно [здесь](https://distill.pub/2016/misread-tsne/) (ещё более красивые картинки!).\n",
        "\n",
        "**Задание** Как и с PCA - воспользуйтесь TSNE из sklearn и нормируйте + центрируйте результат."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-nlN4_aDF9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    scaler = StandardScaler()\n",
        "    tsne = TSNE(n_components=2).fit_transform(word_vectors)\n",
        "    scaler.fit(tsne)\n",
        "    dt = scaler.transform(tsne)\n",
        "    return dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_YFR_rYDK2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_tsne = get_tsne_projection(word_vectors)\n",
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz6uHYMbSjuE",
        "colab_type": "text"
      },
      "source": [
        "## Эмбеддим фразы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4GEypE4SokX",
        "colab_type": "text"
      },
      "source": [
        "Сейчас для тестов будет полезно пользоваться фиксированными эмбеддингами. Загрузим уже обученную модель.  \n",
        "(посмотреть, какие вообще модели и корпуса доступны, можно вызовом `api.info()`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUDRtumXXF3S",
        "colab_type": "code",
        "outputId": "a870d7af-f3da-402b-d0b7-e034d0e5a5c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('glove-twitter-100')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 98.5% 381.3/387.1MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWakpcmphJeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "api.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eypMhlOSXFWN",
        "colab_type": "text"
      },
      "source": [
        "Простой и дешевый способ посчитать эмбеддинг фразы, когда есть эмбеддинги слов - усреднить.\n",
        "\n",
        "Вот этим и займемся: токенизируйте и приведите к нижнему регистру фразу, усредните эмбеддинги тех слов, для которых они подсчитаны.\n",
        "\n",
        "**Задание** Напишите функцию для подсчета эмбеддинга фразы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F76HGRGDEPVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_phrase_embedding(model, phrase):    \n",
        "    if isinstance(phrase, str):\n",
        "      tokens = word_tokenize(phrase.lower())\n",
        "    if isinstance(phrase, list):\n",
        "      tokens = phrase\n",
        "    vector = np.zeros([model.vector_size], dtype='float32')\n",
        "    num_of_embeddings = 0\n",
        "    for token in tokens:\n",
        "      if token in model.vocab:\n",
        "        vector += model.get_vector(token)\n",
        "        num_of_embeddings += 1\n",
        "    if num_of_embeddings != 0:\n",
        "      return vector / num_of_embeddings\n",
        "    else:\n",
        "      return np.zeros([model.vector_size], dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i12XsqnIXfko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector = get_phrase_embedding(model, \"I'm very sure. This never happened to me before...\")\n",
        "\n",
        "\n",
        "assert np.allclose(vector[::10],\n",
        "                   np.array([ 0.30757686, -0.05861897,  0.143751  , -0.11104885, -0.96929336,\n",
        "                             -0.21928601,  0.21652265,  0.14978765,  1.4842536 ,  0.017826  ],\n",
        "                              dtype=np.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl2nBZG0WAUx",
        "colab_type": "text"
      },
      "source": [
        "Подсчитаем вектора всех фраз."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40LezFEJFwv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_vectors = np.array([get_phrase_embedding(model, phrase) for phrase in tokenized_texts])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBj8XMZvWFTv",
        "colab_type": "text"
      },
      "source": [
        "И научимся искать ближайшие!\n",
        "\n",
        "**Задание** По строке найти k ближайших к ней вопросов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjw7kTQ-FP11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_nearest(model, text_vectors, texts, query, k=10):\n",
        "    query_vectors = get_phrase_embedding(model, query).reshape(-1, model.vector_size)\n",
        "    nearest_idx = cosine_similarity(text_vectors, query_vectors).reshape(-1)\n",
        "    nearest_idx = nearest_idx.argsort()[::-1][:k]\n",
        "    return np.take_along_axis(np.array(texts), nearest_idx, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2yK0twNGWaQ",
        "colab_type": "code",
        "outputId": "e88999b4-dea5-4ad2-ed4c-e3786cb7f327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "results = find_nearest(model, text_vectors, texts, query=\"How do i enter the matrix?\", k=10)\n",
        "\n",
        "print('\\n'.join(results))\n",
        "\n",
        "assert len(results) == 10 and isinstance(results[0], str)\n",
        "assert results[1] == 'How do I get to the dark web?'\n",
        "assert results[4] == 'What can I do to save the world?'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How do I download the Mengto's Design-Code book?\n",
            "How do I get to the dark web?\n",
            "What should I do to enter hollywood?\n",
            "How do I use the Greenify app?\n",
            "What can I do to save the world?\n",
            "How do I win this?\n",
            "How do I think out of the box? How do I learn to think out of the box?\n",
            "How do I find the 5th dimension?\n",
            "How do I use the pad in MMA?\n",
            "How do I estimate the competition?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AutTfxbDGhku",
        "colab_type": "code",
        "outputId": "43111f1d-62b3-4112-b7df-536ccad589e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "find_nearest(model, text_vectors, texts, query=\"How does Trump?\", k=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['What does Donald Trump think about Israel?',\n",
              "       'What books does Donald Trump like?',\n",
              "       'What does Donald Trump think of India?',\n",
              "       'What does India think of Donald Trump?',\n",
              "       'What does Donald Trump think of China?',\n",
              "       'What does Donald Trump think about Pakistan?',\n",
              "       'What companies does Donald Trump own?',\n",
              "       'What does Dushka Zapata think about Donald Trump?',\n",
              "       'Does Donald Trump have dementia/alzheimers?',\n",
              "       'How does it feel to date Ivanka Trump?'], dtype='<U1169')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47Ff7heKGiGV",
        "colab_type": "code",
        "outputId": "88101b08-64d8-44e0-9944-a2ddd13ca8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "find_nearest(model, text_vectors, texts, query=\"Why don't i ask a question myself?\", k=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"Why do you always answer a question with a question? I don't, or do I?\",\n",
              "       'Why do I ask this question?', 'How do I ask a question?',\n",
              "       'How do I ask a question on this?',\n",
              "       \"Why do I have to ask a girl out? Why can't she ask me?\",\n",
              "       'How do I downvote a question?', 'How do I ask someone on a date?',\n",
              "       'Why do I question myself about this?',\n",
              "       \"How do I ask a girl I don't know to fuck?\",\n",
              "       'How do you ask a question?'], dtype='<U1169')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BhDEF11ZnTY",
        "colab_type": "text"
      },
      "source": [
        "## Начинаем классификацию"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D_LEV8cm0z3",
        "colab_type": "text"
      },
      "source": [
        "### Bag-of-words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGge73gDid99",
        "colab_type": "text"
      },
      "source": [
        "**Задание** Соберем для начала токенизированные вопросы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJTjqdgiih7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_question1 = [word_tokenize(i.lower()) for i in quora_data.question1]\n",
        "tokenized_question2 = [word_tokenize(j.lower()) for j in quora_data.question2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-yPlmMHnDdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qu1 = np.array([get_phrase_embedding(model, phrase1) for phrase1 in tokenized_question1])\n",
        "qu2 = np.array([get_phrase_embedding(model, phrase2) for phrase2 in tokenized_question2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VBcipE7Ztkc",
        "colab_type": "text"
      },
      "source": [
        "**Задание** Посчитайте косинусную близость между вопросами.\n",
        "\n",
        "Проще всего реализовать функцию ее вычисления руками:\n",
        "$$\\text{cosine_similarity}(x, y) = \\frac{x^{T} y}{||x||\\cdot ||y||}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwoWOu84otZM",
        "colab_type": "code",
        "outputId": "c0742e97-46e5-4ab3-c190-e672a25376c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "quora_data.question1[3], quora_data.question2[3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Why am I mentally very lonely? How can I solve it?',\n",
              " 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItJLR_ENHGtI",
        "colab_type": "code",
        "outputId": "c2a228bf-9064-4de7-d7fd-529942ecd0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "def cos_sim(x, y):\n",
        "    return np.dot(x, y)/(np.linalg.norm(x) * np.linalg.norm(y)) # до этого был х у 3 выражения\n",
        "\n",
        "assert len(qu1) == len(qu2)\n",
        "cosine_similarities = [cos_sim(qu1[i], qu2[i]) for i in range(len(qu2))]\n",
        "assert len(cosine_similarities) == len(qu1) == len(qu1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgCVueJ4Z3DQ",
        "colab_type": "text"
      },
      "source": [
        "Подберём порог близости, при котором будем считать тексты одинаковыми.\n",
        "\n",
        "**Задание** Реализуйте функцию `accuracy`, вычисляющую точность классификации при заданном пороге."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41hRIb-KSA3F",
        "colab_type": "code",
        "outputId": "9781f627-9082-4ebe-81a6-b8728e45736a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def accuracy(cosine_similarities, threshold):\n",
        "    return len([i for i in cosine_similarities if i > threshold])/len(cosine_similarities)\n",
        "\n",
        "thresholds = np.linspace(0, 1, 100, endpoint=False)\n",
        "plt.plot(thresholds, [accuracy(cosine_similarities, th) for th in thresholds])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9e29703eb8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFOgiIjlCgS4ikiP+P2W5QogfMeDhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOxJSE38msmX",
        "colab_type": "text"
      },
      "source": [
        "И запустим оптимизацию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3LVCJwqSgZz",
        "colab_type": "code",
        "outputId": "e51a806c-5c62-416a-bb11-7e09a7d6df86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from scipy.optimize import minimize_scalar\n",
        "\n",
        "res = minimize_scalar(lambda x: -accuracy(cosine_similarities, x), bounds=(0.5, 0.99), method='bounded')\n",
        "best_threshold = res.x\n",
        "print('Threshold = {:.5f}, Accuracy = {:.2%}'.format(best_threshold, accuracy(cosine_similarities, best_threshold)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Threshold = 0.50096, Accuracy = 99.94%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8yFlaffm34A",
        "colab_type": "text"
      },
      "source": [
        "### Tf-idf weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cpeSa-4m64H",
        "colab_type": "text"
      },
      "source": [
        "Вместо того, чтобы тупо усреднять вектора, можно усреднять их с учетом весов - поможет в этом уже знакомые tf-idf.\n",
        "\n",
        "**Задание** Посчитайте взвешенные вектора вопросов. Используйте `TfidfVectorizer`\n",
        "\n",
        "`TfidfVectorizer` возвращает матрицу `(samples_count, words_count)`. А наши эмбеддинги имеют размерность `(words_count, embedding_dim)`. Значит, их можно просто перемножить. Тогда каждая фраза - последовательность слов $w_1, \\ldots, w_k$ - преобразуется в вектор $\\sum_i \\text{idf}(w_i) \\cdot \\text{embedding}(w_i)$. Этот вектор, вероятно, стоит нормировать на число слов $k$.\n",
        "\n",
        "**Задание** Кроме tf-idf можно добавить фильтрацию стоп-слов и пунктуации.  \n",
        "Стоп-слова можно взять из `nltk`:\n",
        "```python\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoYjVExHd_OC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "stop_words = stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Di8jXxoBxg",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим, что получается после фильтрации и векторизации:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k52NtVJKf1Wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in tfidf_question1[0].tocoo().col:\n",
        "    print(model.index2word[col], end=' ')\n",
        "\n",
        "print('\\n' + ' '.join(tokenized_question1[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fMqY5tOoL9r",
        "colab_type": "text"
      },
      "source": [
        "**Задание** Посчитайте качество с новыми векторами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfCYvEQ2bUf4",
        "colab_type": "text"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chf2qy7uhbPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine_similarities = <calc it somehow>\n",
        "\n",
        "res = minimize_scalar(lambda x: -accuracy(cosine_similarities, x), bounds=(0.8, 0.99), method='bounded')\n",
        "best_threshold = res.x\n",
        "print('Threshold = {:.5f}, Accuracy = {:.2%}'.format(best_threshold, accuracy(cosine_similarities, best_threshold)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXi24B78sbsg",
        "colab_type": "text"
      },
      "source": [
        "## Посмотрим внутрь обучения словных эмбеддингов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GSDgSXfswp7",
        "colab_type": "text"
      },
      "source": [
        "Ключевая идея - слово можно определить по контексту, в котором оно встречается:\n",
        "![contexts](https://image.ibb.co/mnQ2uz/2018_09_17_21_07_08.png)\n",
        "*From [cs224n, Lecture 2](http://web.stanford.edu/class/cs224n/lectures/lecture2.pdf)*\n",
        "\n",
        "Смотреть, как всё учится, будем здесь: [https://ronxin.github.io/wevi/](https://ronxin.github.io/wevi/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvqrFUS6vVhh",
        "colab_type": "text"
      },
      "source": [
        "## Запилим пословный машинный перевод!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CXcr-ypzGXg",
        "colab_type": "code",
        "outputId": "8b571ea3-99cd-48c8-dcf4-6be108645c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!wget -O ukr_rus.train.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vAK0SWXUqei4zTimMvIhH3ufGPsbnC_O\"\n",
        "!wget -O ukr_rus.test.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1W9R2F8OeKHXruo2sicZ6FgBJUTJc8Us_\"\n",
        "!wget -O fairy_tale.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1sq8zSroFeg_afw-60OmY8RATdu_T1tej\"\n",
        "\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1d7OXuil646jUeDS1JNhP9XWlZogv6rbu'})\n",
        "downloaded.GetContentFile('cc.ru.300.vec.zip')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1yAqwqgUHtMSfGS99WLGe5unSCyIXfIxi'})\n",
        "downloaded.GetContentFile('cc.uk.300.vec.zip')\n",
        "\n",
        "!unzip cc.ru.300.vec.zip\n",
        "!unzip cc.uk.300.vec.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cc.ru.300.vec.zip\n",
            "  inflating: cc.ru.300.vec           \n",
            "Archive:  cc.uk.300.vec.zip\n",
            "  inflating: cc.uk.300.vec           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RqUeOXxws8y",
        "colab_type": "text"
      },
      "source": [
        "Напишем простенькую реализацию модели машинного перевода.\n",
        "\n",
        "Идея основана на статье [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087.pdf). У авторов в репозитории еще много интересного: [https://github.com/facebookresearch/MUSE](https://github.com/facebookresearch/MUSE).\n",
        "\n",
        "А мы будем переводить с украинского на русский.\n",
        "\n",
        "![](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/blue_cat_blue_whale.png)   \n",
        "*синій кіт* vs. *синій кит*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjPj9FTRry0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")\n",
        "uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rGx4TXWFJ65",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на пару серпень-август (являющихся переводом)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkHer36xyh4n",
        "colab_type": "code",
        "outputId": "10c49d59-af3a-488b-9189-5ae50efee969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "ru_emb.most_similar([ru_emb[\"август\"]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('август', 1.0),\n",
              " ('июль', 0.9383153915405273),\n",
              " ('сентябрь', 0.9240028858184814),\n",
              " ('июнь', 0.9222575426101685),\n",
              " ('октябрь', 0.9095538854598999),\n",
              " ('ноябрь', 0.8930036425590515),\n",
              " ('апрель', 0.8729087114334106),\n",
              " ('декабрь', 0.8652557730674744),\n",
              " ('март', 0.8545796275138855),\n",
              " ('февраль', 0.8401416540145874)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RSDixWvylEP",
        "colab_type": "code",
        "outputId": "83630d88-88e2-4630-80c7-d70b25c530c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "uk_emb.most_similar([uk_emb[\"серпень\"]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('серпень', 0.9999999403953552),\n",
              " ('липень', 0.9096440076828003),\n",
              " ('вересень', 0.901697039604187),\n",
              " ('червень', 0.8992519378662109),\n",
              " ('жовтень', 0.8810408711433411),\n",
              " ('листопад', 0.8787633776664734),\n",
              " ('квітень', 0.8592804670333862),\n",
              " ('грудень', 0.8586863279342651),\n",
              " ('травень', 0.8408110737800598),\n",
              " ('лютий', 0.8256431818008423)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwmm3YQ1yl1U",
        "colab_type": "code",
        "outputId": "57c966f6-093f-40a2-90fe-c4b591644c53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "ru_emb.most_similar([uk_emb[\"серпень\"]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Недопустимость', 0.24435284733772278),\n",
              " ('конструктивность', 0.23293080925941467),\n",
              " ('офор', 0.23256804049015045),\n",
              " ('deteydlya', 0.23031717538833618),\n",
              " ('пресечении', 0.22632381319999695),\n",
              " ('одностороннего', 0.22608885169029236),\n",
              " ('подход', 0.2230587601661682),\n",
              " ('иболее', 0.22003726661205292),\n",
              " ('2015Александр', 0.21872764825820923),\n",
              " ('конструктивен', 0.21796566247940063)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCU6bZIdVAKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAsW7oxszE_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_word_pairs(filename):\n",
        "    uk_ru_pairs = []\n",
        "    uk_vectors = []\n",
        "    ru_vectors = []\n",
        "    with open(filename, \"r\", encoding='utf8') as inpf:\n",
        "        for line in inpf:\n",
        "            uk, ru = line.rstrip().split(\"\\t\")\n",
        "            if uk not in uk_emb or ru not in ru_emb:\n",
        "                continue\n",
        "            uk_ru_pairs.append((uk, ru))\n",
        "            uk_vectors.append(uk_emb[uk])\n",
        "            ru_vectors.append(ru_emb[ru])\n",
        "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)\n",
        "\n",
        "\n",
        "uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")\n",
        "uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z6ts7DC0XmN",
        "colab_type": "text"
      },
      "source": [
        "### Учим маппинг из одного пространства эмбеддингов в другое\n",
        "\n",
        "У нас есть пары слов, соответствующих друг другу, и их эмбеддинги. Найдем преобразование из одного пространства в другое, чтобы приблизить известные нам слова:\n",
        "\n",
        "$$W^*= \\arg\\min_W ||WX - Y||_F, \\text{где} ||*||_F - \\text{норма Фробениуса}$$\n",
        "\n",
        "Эта функция очень похожа на линейную регрессию (без биаса).\n",
        "\n",
        "**Задание** Реализуйте её - воспользуйтесь `LinearRegression` из sklearn с `fit_intercept=False`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fraTOQtu1YWI",
        "colab_type": "code",
        "outputId": "31c5fabf-7e3c-480d-fe13-ef7d3337a3fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "mapping = LinearRegression(fit_intercept=False)\n",
        "mapping.fit(X_train, Y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrzRk3ja1b_6",
        "colab_type": "text"
      },
      "source": [
        "Проверим, куда перейдет `серпень`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Quax6HnF1aON",
        "colab_type": "code",
        "outputId": "0e5e866f-73a8-4fd2-c7e7-d5e9fb54f0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
        "ru_emb.most_similar(august)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('апрель', 0.8541285991668701),\n",
              " ('июнь', 0.8411202430725098),\n",
              " ('март', 0.839699387550354),\n",
              " ('сентябрь', 0.835986852645874),\n",
              " ('февраль', 0.8329297304153442),\n",
              " ('октябрь', 0.8311845660209656),\n",
              " ('ноябрь', 0.8278923034667969),\n",
              " ('июль', 0.8234529495239258),\n",
              " ('август', 0.8120501041412354),\n",
              " ('декабрь', 0.803900420665741)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUaGc0jaRgNe",
        "colab_type": "code",
        "outputId": "45f5b46a-4756-4398-c7a2-0faa5ed0a192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "type(august), august.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, (1, 300))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih1GLNZt1nZX",
        "colab_type": "text"
      },
      "source": [
        "Должно получиться, что в топе содержатся разные месяцы, но август не первый.\n",
        "\n",
        "Будем мерять percision top-k с k = 1, 5, 10.\n",
        "\n",
        "**Задание** Реализуйте следующую функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnmrLp9y2gNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# кажется, можно сделать проще\n",
        "def precision(pairs, mapped_vectors, topn=1):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
        "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
        "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
        "    :returns:\n",
        "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
        "    \"\"\"\n",
        "    assert len(pairs) == len(mapped_vectors)\n",
        "    num_matches = 0\n",
        "    if mapped_vectors[0].shape[0] != 1:\n",
        "            map_vec = [j.reshape(1, -1) for j in mapped_vectors]\n",
        "    for i, (_, ru) in enumerate(pairs):\n",
        "        for (s, _) in ru_emb.most_similar(map_vec[i], topn=topn):\n",
        "            if s == ru:\n",
        "                num_matches += 1\n",
        "                break\n",
        "\n",
        "    precision_val = num_matches / len(pairs)\n",
        "    return precision_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1NIvhSH2olG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
        "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
        "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ml_w1Tl2r7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert precision(uk_ru_test, X_test) == 0.0\n",
        "assert precision(uk_ru_test, Y_test) == 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d9KQHMr2tx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
        "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)\n",
        "\n",
        "assert precision_top1 >= 0.635\n",
        "assert precision_top5 >= 0.813"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNbDTP502urT",
        "colab_type": "text"
      },
      "source": [
        "### Улучшаем маппинг\n",
        "\n",
        "Можно показать, что маппинг лучше строить ортогональным:\n",
        "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, где: } W^TW = I$$\n",
        "\n",
        "Искать его можно через SVD:\n",
        "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
        "\n",
        "$$W^*=UV^T$$\n",
        "\n",
        "**Задание** Реализуйте эту функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA0mN2XC8V8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_transform(X_train, Y_train):\n",
        "    \"\"\" \n",
        "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
        "    \"\"\"\n",
        "    M = X_train.T @ Y_train\n",
        "    U, sigma, Vh = np.linalg.svd(M, full_matrices=True)\n",
        "    w = U @ Vh # если транспонировать Vh , то не получается \n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BLDATk9Iy29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = learn_transform(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-goVl41y_vsE",
        "colab_type": "code",
        "outputId": "ec51f9ec-0c46-423e-94ef-74c56d50dd96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "W.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7HXbQUuKQJP",
        "colab_type": "code",
        "outputId": "e052f7ea-5e7b-4077-c06a-52b57fa6d11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "ru_emb.most_similar([np.matmul(uk_emb[\"дівчина\"], W)])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('девушка', 0.8674468398094177),\n",
              " ('женщина', 0.7635207176208496),\n",
              " ('девчонка', 0.7521330118179321),\n",
              " ('девочка', 0.7488856911659241),\n",
              " ('девица', 0.7135095000267029),\n",
              " ('девка', 0.7049036026000977),\n",
              " ('красавица', 0.7047942876815796),\n",
              " ('красотка', 0.6963311433792114),\n",
              " ('барышня', 0.6869029998779297),\n",
              " ('незнакомка', 0.680800199508667)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDQz6NmDjNC-",
        "colab_type": "code",
        "outputId": "a2b0449d-b8fe-486b-c991-80baf91f4296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "precision(uk_ru_test, np.matmul(X_test, W))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6537467700258398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nn58crh4AH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert precision(uk_ru_test, np.matmul(X_test, W)) >= 0.653\n",
        "assert precision(uk_ru_test, np.matmul(X_test, W), 5) >= 0.824"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqgcYk-c4DE5",
        "colab_type": "text"
      },
      "source": [
        "### Пишем переводчик"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwi70fP6FaAN",
        "colab_type": "text"
      },
      "source": [
        "Реализуем простой пословный переводчик - для каждого слова будем искать его ближайшего соседа в общем пространстве эмбеддингов. Если слова нет в эмбеддингах - просто копируем его."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj3NiywDNa5M",
        "colab_type": "code",
        "outputId": "8cf5dc9c-5350-4e31-d2a5-2f203af1fb12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "ru_emb.most_similar([np.matmul(uk_emb[\"1\"], W)])[0][0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0etAHUks4JOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"fairy_tale.txt\", \"r\") as f:\n",
        "    uk_sentences = [line.rstrip().lower() for line in f]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK_FJGmn4N7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        sentence - sentence in Ukrainian (str)\n",
        "    :returns:\n",
        "        translation - sentence in Russian (str)\n",
        "\n",
        "    * find ukrainian embedding for each word in sentence\n",
        "    * transform ukrainian embedding vector\n",
        "    * find nearest russian word and replace\n",
        "    \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.split(' ')\n",
        "    translation = []\n",
        "    for i in sentence:\n",
        "        if i in uk_emb:\n",
        "            translation.append(ru_emb.most_similar([np.matmul(uk_emb[i], W)])[0][0])\n",
        "        else:\n",
        "            translation.append(i)\n",
        "    return ' '.join(translation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxMRVTb6O92U",
        "colab_type": "code",
        "outputId": "3a00db56-7f6d-4bbd-ba19-f5adb3a4235c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "translate(\"Як справи , друже ?\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'как дела , дружище ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H47pbFyk4P6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert translate(\".\") == \".\"\n",
        "assert translate(\"1 , 3\") == \"1 , 3\"\n",
        "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAVWK7mE4RYU",
        "colab_type": "code",
        "outputId": "adb54a98-e5aa-451f-906b-1ddd2313d546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "for sentence in uk_sentences:\n",
        "    print(\"src: {}\\ndst: {}\\n\".format(sentence, translate(sentence)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src: лисичка - сестричка і вовк - панібрат\n",
            "dst: лисичка – сестричка и волк – панібрат\n",
            "\n",
            "src: як була собі лисичка , да й пішла раз до однії баби добувать огню ; ввійшла у хату да й каже : \" добрий день тобі , бабусю !\n",
            "dst: как была себе лисичка , че и пошла раз к однії бабы добувать огня ; вошла во избу че и говорит : \" хороший день тебе , бабушку !\n",
            "\n",
            "src: дай мені огня \" .\n",
            "dst: дай мне огня \" .\n",
            "\n",
            "src: а баба тільки що вийняла із печі пирожок із маком , солодкий , да й положила , щоб він прохолов ; а лисичка се і підгледала , да тілько що баба нахилилась у піч , щоб достать огня , то лисичка зараз ухватила пирожок да і драла з хати , да , біжучи , весь мак із його виїла , а туда сміття наклала .\n",
            "dst: а бабка только что вынула со печи пирожок со маком , сладкий , че и согнула , чтобы он прохолов ; а лисичка ой и підгледала , че токмо что бабка качнулась во печь , чтобы достать огня , то лисичка сейчас ухватила пирожок че и деру со хаты , че , пробежать , весь мак со его виїла , а туда мусора наложила .\n",
            "\n",
            "src: прибігла на поле , аж там пасуть хлопці бичків .\n",
            "dst: прибежала по поле , аж там пасут парни бычков .\n",
            "\n",
            "src: вона і каже їм : \" ей , хлопці !\n",
            "dst: она и говорит им : \" ой , парни !\n",
            "\n",
            "src: проміняйте мені бичка - третячка за маковий пирожок \" .\n",
            "dst: проміняйте мне бычка – третячка за маковый пирожок \" .\n",
            "\n",
            "src: тії согласились ; так вона їм говорить : \" смотріть же , ви не їжте зараз сього пирожка , а тоді уже розломите , як я заведу бичка за могилку ; а то ви його ні за що не розломите \" .\n",
            "dst: ишо поглумиться ; так она им говорит : \" смотріть то , мы не ешьте сейчас сего пирожка , а тогда уже розломите , как мной заведу бычка за могилу ; а то мы его ни за что не розломите \" .\n",
            "\n",
            "src: бачите вже - лисичка таки собі була розумна , що хоть кого да обманить .\n",
            "dst: вижу уже – лисичка таки себе была умная , что хоть кого че обманить .\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-7af83f311978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muk_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src: {}\\ndst: {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-86-4e967e08a321>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muk_emb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mru_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muk_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mlimited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrestrict_vocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimited\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChsdWo8RSrF1",
        "colab_type": "code",
        "outputId": "954677c7-5f19-4db7-ea82-336bd9f45039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "ru_emb.most_similar([np.matmul(uk_emb[\"Зеленський\"], W)])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Зеленский', 0.48910245299339294),\n",
              " ('кремлевский', 0.452633798122406),\n",
              " ('Кремлевский', 0.4447115361690521),\n",
              " ('Покровский', 0.437208354473114),\n",
              " ('Путинский', 0.43487831950187683),\n",
              " ('Разумовский', 0.4303314685821533),\n",
              " ('Павловский', 0.4270281195640564),\n",
              " ('кремлёвский', 0.4228673577308655),\n",
              " ('Майский', 0.41998565196990967),\n",
              " ('Деревенский', 0.4197758734226227)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grf2Ly8jVmYg",
        "colab_type": "text"
      },
      "source": [
        "¯\\_(ツ)_/¯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDnIPVLg4Sg7",
        "colab_type": "text"
      },
      "source": [
        "# Сдача задания\n",
        "\n",
        "[Форма для сдачи](https://goo.gl/forms/GGjrH7axdGJr6yTp2)  \n",
        "[Опрос](https://goo.gl/forms/3QRwLTmLgBzl5VVm2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5GrChTeFqIg",
        "colab_type": "text"
      },
      "source": [
        "# Дополнительные материалы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwffxpbmFwDh",
        "colab_type": "text"
      },
      "source": [
        "## Почитать\n",
        "### База:  \n",
        "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
        "[Deep Learning, NLP, and Representations, Christopher Olah](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)  \n",
        "\n",
        "### Как кластеризовать смыслы многозначных слов:  \n",
        "[Making Sense of Word Embeddings (2016), Pelevina et al](http://anthology.aclweb.org/W16-1620)    \n",
        "\n",
        "### Как оценивать эмбеддинги\n",
        "[Evaluation methods for unsupervised word embeddings (2015), T. Schnabel](http://www.aclweb.org/anthology/D15-1036)  \n",
        "[Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance (2016), B. Chiu](https://www.aclweb.org/anthology/W/W16/W16-2501.pdf)  \n",
        "[Problems With Evaluation of Word Embeddings Using Word Similarity Tasks (2016), M. Faruqui](https://arxiv.org/pdf/1605.02276.pdf)  \n",
        "[Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure (2016), Oded Avraham, Yoav Goldberg](https://arxiv.org/pdf/1611.03641.pdf)  \n",
        "[Evaluating Word Embeddings Using a Representative Suite of Practical Tasks (2016), N. Nayak](https://cs.stanford.edu/~angeli/papers/2016-acl-veceval.pdf)  \n",
        "\n",
        "\n",
        "## Посмотреть\n",
        "[Word Vector Representations: word2vec, Lecture 2, cs224n](https://www.youtube.com/watch?v=ERibwqs9p38)"
      ]
    }
  ]
}